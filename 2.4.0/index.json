[
{
	"uri": "/fmw-kubernetes/soa-domains/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Sample for creating a SOA Suite domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated SOA domain.",
	"content": "Introduction The operator supports deployment of SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS). Currently the operator supports these different domain types:\n soa: Deploys a SOA domain osb: Deploys an OSB (Oracle Service Bus) domain soaess: Deploys a SOA domain with Enterprise Scheduler (ESS) soaosb: Deploys a domain with SOA and OSB soaessosb: Deploys a domain with SOA, OSB, and ESS  This document provides information about the system requirements and limitations for deploying and running SOA Suite domains with the operator.\nIn this release, SOA Suite domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nSystem requirements for SOA Suite domains  Kubernetes 1.13.5+, 1.14.3+ and 1.15.2+ (check with kubectl version). Flannel networking v0.11.0-amd64 (check with docker images | grep flannel). Docker 18.9.1 (check with docker version) Helm 2.14.0+ (check with helm version). Oracle Fusion Middleware Infrastructure 12.2.1.4.0 image container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4. You must have the cluster-admin role to install the operator. We do not currently support running SOA in non-Linux containers.  Limitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for SOA Suite domains:\n The \u0026ldquo;domain in image\u0026rdquo; model is not supported. Only configured clusters are supported. Dynamic clusters are not supported for SOA Suite domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. Deploying and running SOA Suite domains is supported only in operator versions 2.4.0 and later. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  Oracle SOA Cluster Sizing Recommendations    Oracle SOA Normal Usage Moderate Usage High Usage     Admin Server No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB   Managed Server No of Servers : 2, No of CPU(s) : 2, Memory : 16GB No of Servers : 2, No of CPU(s) : 4, Memory : 16GB No of Servers : 3, No of CPU(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/",
	"title": "Oracle Fusion Middleware on Kubernetes",
	"tags": [],
	"description": "This document lists all the Oracle Fusion Middleware products deployment supported on Kubernetes.",
	"content": "Oracle Fusion Middleware on Kubernetes Oracle supports the deployment of the following Oracle Fusion Middleware products on Kubernetes. Click on the appropriate document link below to get started on setting up the product.\n Oracle SOA Suite  The WebLogic Kubernetes operator supports deployment of SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS). Follow the instructions in this guide to set up these Oracle SOA Suite domains on Kubernetes.\n Oracle WebCenter Sites  The WebLogic Kubernetes operator supports deployment of Oracle WebCenter Sites. Follow the instructions in this guide to set up Oracle WebCenter Sites domains on Kubernetes.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/manage-soa-domains/configuring-load-balancer/",
	"title": "Configuring a load balancer for SOA Domains",
	"tags": [],
	"description": "Learn about configuring an Ingress based load balancer for SOA domains.",
	"content": "An Ingress based load balancer can be configured to access the Oracle SOA and Oracle Service Bus domain application URLs. Refer to the setup Ingress document for details.\nAs part of the ingress-per-domain setup for Oracle SOA and Oracle Service Bus domains, the values.yaml file (under the ingress-per-domain directory) needs to be updated with the appropriate values from your environment. A sample values.yaml file (for the Traefik load balancer) is shown below:\n# Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: TRAEFIK, VOYAGER type: TRAEFIK # WLS domain as backend to the load balancer wlsDomain: domainUID: soainfra soaClusterName: soa_cluster osbClusterName: osb_cluster soaManagedServerPort: 8001 osbManagedServerPort: 9001 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values traefik: # hostname used by host-routing hostname: testhost.domain.com # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315 Below are the path-based, Ingress routing rules (spec.rules section) that need to be defined for Oracle SOA and Oracle Service Bus domains. You need to update the appropriate Ingress template YAML file based on the load balancer being used. For example, the template YAML file for the Traefik load balancer is located at kubernetes/samples/charts/ingress-per-domain/templates/traefik-ingress.yaml.\nrules: - host: \u0026#39;{{ .Values.traefik.hostname }}\u0026#39; http: paths: - path: /console backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /em backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /servicebus backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /lwpfconsole backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.soaClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.soaManagedServerPort }} Now you can access the Oracle SOA Suite domain URLs, as listed below, based on the domain type you selected.\n  Oracle SOA:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/weblogic/ready http://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/console http://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/em http://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/soa-infra http://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/soa/composer http://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/integration/worklistapp\n  Oracle Enterprise Scheduler Service (ESS):\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/ess http://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/EssHealthCheck\n  Oracle Service Bus (OSB):\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/servicebus http://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt;/lwpfconsole\n  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/pre-requisites/",
	"title": "Pre-requisites ",
	"tags": [],
	"description": "Pre-requisites for setting up WebCenter Sites domains with WebLogic Kubernetes Operator",
	"content": "Contents  Introduction System Requirements Limitations WebCenter Sites Cluster Sizing Recommendations  Introduction This document describes the special considerations for deploying and running a WebCenter Sites domain with the WebLogic Kubernetes Operator. Other than those considerations listed here, WebCenter Sites domains work in the same way as Fusion Middleware Infrastructure domains and WebLogic Server domains.\nIn this release, WebCenter Sites domains are supported using the domain on a persistent volume model only where a WebCenter Sites domain is located in a persistent volume (PV).\nSystem Requirements  Kubernetes 1.13.0+, 1.14.0+, and 1.15.0+ (check with kubectl version). Flannel networking v0.9.1-amd64 (check with docker images | grep flannel). Docker 18.9.1 (check with docker version) Helm 2.14.3+ (check with helm version). Oracle Fusion Middleware Infrastructure 12.2.1.4.0 image. You must have the cluster-admin role to install the operator. These proxy setup are used for pulling the required binaries and source code from the respective repositories:  export NO_PROXY=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export no_proxy=\u0026quot;localhost,127.0.0.0/8,$(hostname -i),.your-company.com,/var/run/docker.sock\u0026rdquo; export http_proxy=http://www-proxy-your-company.com:80 export https_proxy=http://www-proxy-your-company.com:80 export HTTP_PROXY=http://www-proxy-your-company.com:80 export HTTPS_PROXY=http://www-proxy-your-company.com:80    NOTE: Add your host IP by using hostname -i and also nslookup IP addresses to the no_proxy, NO_PROXY list above.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the Operator, the following limitations currently exist for WebCenter Sites domain:\n Domain in image model is not supported in this version of the Operator. Only configured clusters are supported. Dynamic clusters are not supported for WebCenter Sites domains. Note that you can still use all of the scaling features. You just need to define the maximum size of your cluster at domain creation time. We do not currently support running WebCenter Sites in non-Linux containers. Deploying and running a WebCenter Sites domain is supported only in Operator versions 2.4.0 and later. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  WebCenter Sites Cluster Sizing Recommendations    WebCenter Sites Normal Usage Moderate Usage High Usage     Admin Server No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB No of CPU(s) : 1, Memory : 4GB   Managed Server No of Servers : 2, No of CPU(s) : 2, Memory : 16GB No of Servers : 2, No of CPU(s) : 4, Memory : 16GB No of Servers : 3, No of CPU(s) : 6, Memory : 16-32GB   PV Storage Minimum 250GB Minimum 250GB Minimum 500GB    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "SOA domains include the deployment of various Oracle Service-Oriented Architecture (SOA) Suite components, such as SOA, Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS).",
	"content": " Oracle SOA Suite is currently supported only for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle SOA Suite in Kubernetes before it is supported for production use.\n  Set up your Kubernetes cluster Install Helm and Tiller Get dependent Images Set up the code repository to deploy Oracle SOA Suite domains Obtaining the SOA Suite Docker image Creating a SOA Suite Docker image Creating a custom SOA Suite Docker image using Imagetool Install the WebLogic Kubernetes Operator Prepare the environment for Oracle SOA Suite Domains  Create a namespace for SOA Domain Create a persistent storage for SOA Domain Create a secret with domain credentials Create a Kubernetes secret with the RCU credentials Configuring access to your database Running the Repository Creation Utility to set up your database schemas   Creating a SOA domain  Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Install Helm and Tiller The operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see Install Helm and Tiller.\nGet dependent Images Get these images and put them into your local registry.\n  If you don\u0026rsquo;t already have one, obtain a Docker store account, log in to the Docker store and accept the license agreement for the WebLogic Server image.\n  Log in to the Docker store from your Docker client:\n$ docker login   Pull the operator image:\n$ docker pull oracle/weblogic-kubernetes-operator:2.4.0   Pull the Traefik load balancer image:\n$ docker pull traefik:1.7.12   Pull the Oracle Database image:\n$ docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim $ docker tag container-registry.oracle.com/database/enterprise:12.2.0.1-slim oracle/database:12.2.0.1   Set up the code repository to deploy Oracle SOA Suite domains Oracle SOA Suite domains deployment on Kubernetes leverages the Oracle WebLogic Kubernetes Operator infrastructure. For deploying the Oracle SOA Suite domains, you need to set up the deployment scripts as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; $ cd \u0026lt;work directory\u0026gt;   Download the supported version of Oracle WebLogic Kubernetes Operator source code archieve file (.zip/.tar.gz) from the operator relases page. Currently the supported operator version is 2.4.0.\n  Extract the source code archive file (.zip/.tar.gz) in to the work directory.\n  Download the SOA Suite kubernetes deployment scripts from the SOA repository and copy them in to WebLogic operator samples location.\n$ git clone https://orahub.oraclecorp.com/tooling/fmw-kubernetes.git $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleSOASuite/kubernetes/2.4.0/create-soa-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0/kubernetes/samples/scripts/   You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0/kubernetes/samples/scripts/ to set up the SOA Suite domains as further described in this document.\nObtaining the SOA Suite Docker Image The pre-built Oracle SOA Suite image is available at, container-registry.oracle.com/middleware/soasuite:12.2.1.4 (TBD).\nTo pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nTo obtain the image, log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com Then, you can pull the image with this command:\n$ docker pull container-registry.oracle.com/middleware/soasuite:12.2.1.4 Creating a SOA Suite Docker image You can also create a Docker image containing the Oracle SOA Suite binaries. This is the recommended approach if you have access to the Oracle SOA bundle patch.\nPlease consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle FMW Infrastructure Docker image, and downloading the Oracle SOA Suite installer and bundle patch binaries.\nA pre-built Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle SOA Suite image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 oracle/fmw-infrastructure:12.2.1.4 Follow these steps to build the necessary images - a Fusion Middleware Infrastructure image, and then the SOA Suite image as a layer on top of that:\n  Make a local clone of the sample repository.\n$ git clone https://github.com/oracle/docker-images   Build the oracle/fmw-infrastructure:12.2.1.4 image as shown below:\n$ cd docker-images/OracleFMWInfrastructure/dockerfiles $ sh buildDockerImage.sh -v 12.2.1.4 -s This will produce an image named oracle/fmw-infrastructure:12.2.1.4.\n  Download the Oracle SOA Suite installer, latest Oracle SOA bundle patch (30638101 or later) from the Oracle Technology Network or e-delivery.\n NOTE: Copy the installer binaries to the same location as the Dockerfile and the patch ZIP files under the docker-images/OracleSOASuite/dockerfiles/12.2.1.4/patches folder.\n   Create the Oracle SOA Suite image by running the provided script:\n$ cd docker-images/OracleSOASuite/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/soa:12.2.1.4. The samples and instructions assume the Oracle SOA Suite image is named container-registry.oracle.com/middleware/soasuite:12.2.1.4. You will need to rename your image to match this name, or update the samples to refer to the image you created.\n$ docker tag oracle/soa:12.2.1.4 container-registry.oracle.com/middleware/soasuite:12.2.1.4 You can use this image to run the Repository Creation Utility and to run your domain using the “domain on a persistent volume” model.\n  Creating a custom SOA Suite Docker image using Imagetool Steps: To be verified and added here.\nInstall the WebLogic Kubernetes Operator The WebLogic Kubernetes Operator supports the deployment of Oracle SOA Suite domains in Kubernetes environment. Follow the steps in this document to install the operator.\nFor early access customers, with bundle patch access, we recommend that you build and use the Oracle SOA Suite Docker image with the latest bundle patch for Oracle SOA. The Oracle SOA Suite Docker image in container-registry.oracle.com does not have the bundle patch installed. However, if you do not have access to the bundle patch, you can obtain the Oracle SOA Suite Docker image without the bundle patch from container-registry.oracle.com, as described below.\n Prepare the environment for Oracle SOA Suite Domains Create a namespace for SOA Domain Create a Kubernetes namespace (for example, soans) for the domain unless you intend to use the default namespace. Use the newly created namespace in all the other steps. For details, see Prepare to run a domain.\n $ kubectl create namespace soans Create a persistent storage for SOA Domain In the Kubernetes namespace created above, create the PV and PVC for the database by running the create-pv-pvc.sh script. Follow the instructions for using the scripts to create a PV and PVC.\n* Change the values in the [create-pv-pvc-inputs.yaml](https://github.com/oracle/weblogic-kubernetes-operator/blob/master/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/create-pv-pvc-inputs.yaml) file based on your requirements. * Ensure that the path for the `weblogicDomainStoragePath` property exists (if not, you need to create it), has full access permissions, and that the folder is empty.  Create a secret with domain credentials Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain. For details, see this document.\n```bash $ cd kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p Welcome1 -n soans -d soainfra -s soainfra-domain-credentials ``` You can check the secret with the `kubectl get secret` command. See the following example, including the output: ```bash $ kubectl get secret soainfra-domain-credentials -o yaml -n soans apiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: 2019-06-02T07:05:25Z labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra name: soainfra-domain-credentials namespace: soans resourceVersion: \u0026quot;11561988\u0026quot; selfLink: /api/v1/namespaces/soans/secrets/soainfra-domain-credentials uid: a91ef4e1-6ca8-11e9-8143-fa163efa261a type: Opaque ```  Create a Kubernetes secret with the RCU credentials You also need to create a Kubernetes secret containing the credentials for the database schemas. When you create your domain using the sample provided below, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret as shown below:\n$ cd kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh \\  -u SOA1_SOAINFRA \\  -p Welcome1 \\  -a sys \\  -q Oradoc_db1 \\  -d soainfra \\  -n soans \\  -s soainfra-rcu-credentials The parameter values are as follows:\n The schema owner user name (-u) must be the schemaPrefix value followed by an underscore and a component name, for example SOA1_SOAINFRA. The schema owner password (-p) will be the password you provided for regular schema users during RCU creation. The database administration user (-u) and password (-q). The domainUID for the domain (-d). The namespace the domain is in (-n); if omitted, then default is assumed. The name of the secret (-s).  You can confirm the secret was created as expected with the kubectl get secret command. An example is shown below, including the output:\n$ kubectl get secret soainfra-rcu-credentials -o yaml -n soans apiVersion: v1 data: password: V2VsY29tZTE= sys_password: T3JhZG9jX2RiMQ== sys_username: c3lz username: U09BMQ== kind: Secret metadata: creationTimestamp: 2019-06-02T07:15:31Z labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra name: soainfra-rcu-credentials namespace: soans resourceVersion: \u0026#34;11562794\u0026#34; selfLink: /api/v1/namespaces/soans/secrets/soainfra-rcu-credentials uid: 1230385e-6caa-11e9-8143-fa163efa261a type: Opaque Configuring access to your database SOA Suite domains require a database with the necessary schemas installed in them. The Repository Creation Utility (RCU) allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running SOA in Kubernetes; the same existing requirements apply.\nFor testing and development, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nThe Oracle Database Docker images are supported for non-production use only. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).\n Running the database inside Kubernetes Follow these instructions to perform a basic deployment of the Oracle database in Kubernetes. For more details about database setup and configuration, refer to this page.\nWhen running the Oracle database in Kubernetes, you have an option to attach persistent volumes (PV) so that the database storage will be persisted across database restarts. If you prefer not to persist the database storage, follow the instructions in this document to set up a database in a container with no persistent volume (PV) attached.\n NOTE: start-db-service.sh creates the database in the default namespace. If you want to create the database in a different namespace, you need to manually update the value for all the occurrences of the namespace field in the provided sample file create-rcu-schema/common/oracle.db.yaml.\n These instructions will set up the database in a container with the persistent volume (PV) attached. If you chose not to use persistent storage, please go to the RCU creation step.\n Create the persistent volume and persistent volume claim for the database using the create-pv-pvc.sh sample. Refer to the instructions provided in that sample.  When creating the PV and PVC for the database, make sure that you use a different name and storage class for the PV and PVC for the domain. The name is set using the value of the baseName field in create-pv-pvc-inputs.yaml.\n  Start the database and database service using the following commands:   NOTE: Make sure you update the kubernetes/samples/scripts/create-soa-domain/domain-home-on-pv/create-database/db-with-pv.yaml file with the name of the PVC created in the previous step. Also, update the value for all the occurrences of the namespace field to the namespace where the database PVC was created.\n ```bash $ cd weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain/domain-home-on-pv/create-database $ kubectl create -f db-with-pv.yaml ```  The database will take several minutes to start the first time, while it performs setup operations. You can watch the log to see its progress using this command:\n$ kubectl logs -f oracle-db -n soans A log message will indicate when the database is ready. Also, you can verify the database service status using this command:\n$ kubectl get pods,svc -n soans |grep oracle-db po/oracle-db 1/1 Running 0 6m svc/oracle-db ClusterIP None \u0026lt;none\u0026gt; 1521/TCP,5500/TCP 7m Before creating a domain, you will need to set up the necessary schemas in your database.\nRunning the Repository Creation Utility to set up your database schemas Creating schemas To create the database schemas for Oracle SOA Suite, run the create-rcu-schema.sh script as described here.\nThe following example shows commands you might use to execute create-rcu-schema.sh:\n$ cd weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./create-rcu-schema.sh \\  -s SOA1 \\  -t soaessosb \\  -d oracle-db.soans.svc.cluster.local:1521/devpdb.k8s \\  -i container-registry.oracle.com/middleware/soasuite:12.2.1.4 For SOA domains, the create-rcu-schema.sh script supports the following domain types soa,osb,soaosb,soaess,soaessosb. You must specify one of these using the -t flag.\nYou need to make sure that you maintain the association between the database schemas and the matching domain just like you did in a non-Kubernetes environment. There is no specific functionality provided to help with this.\nDropping schemas If you want to drop the schema, you can use the drop-rcu-schema.sh script as described here.\nThe following example shows commands you might use to execute drop-rcu-schema.sh:\n$ cd weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./drop-rcu-schema.sh \\  -s SOA1 \\  -t soaessosb \\  -d oracle-db.soans.svc.cluster.local:1521/devpdb.k8s For SOA domains, the drop-rcu-schema.sh script supports the domain types soa,osb,soaosb,soaess,soaessosb. You must specify one of these using the -t flag.\nCreating a SOA domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the instructions in Create SOA Domains.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/manage-soa-domains/monitoring-soa-domains/",
	"title": "Monitoring a SOA domain",
	"tags": [],
	"description": "Describes the steps for Monitoring the SOA domain and Publising the logs to Elasticsearch.",
	"content": "After the SOA domain is set up, you can:\n Monitor the SOA instance using Prometheus and Grafana. See Monitoring a domain. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. See Publish logs to Elasticsearch.  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/prepare-your-environment/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "",
	"content": "Contents This document describes the steps to set up the environment that includes setting up of a Kubernetes cluster and setting up the Weblogic Operator including the database.\n Introduction Set Up your Kubernetes Cluster Build Oracle WebCenter Sites Image Pull Other Dependent Images Set Up the Code Repository to Deploy Oracle WebCenter Sites Domain Grant Roles and Clear Stale Resources Install the WebLogic Kubernetes Operator Configure NFS Server Prepare the Environment for the WebCenter Sites Domain Set Up the Database  Introduction Set Up your Kubernetes Cluster If you need help in setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Build Oracle WebCenter Sites Image Build Oracle WebCenter Sites 12.2.1.4.0 Image by following steps 4A, 4C, 4D and 5 from this document.\nPull Other Dependent Images Dependent images include WebLogic Kubernetes Operator, Database, and Traefik. Pull these images and add them to your local registry:\n Pull these docker images and re-tag them as shown:  To pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThen, pull these docker images and re-tag them:\ndocker login https://container-registry.oracle.com (enter your Oracle email Id and password) This step is required once at every node to get access to the Oracle Container Registry. WebLogic Kubernetes Operator image:\n$ docker pull oracle/weblogic-kubernetes-operator:2.4.0 Database image:\n$ docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim $ docker tag container-registry.oracle.com/database/enterprise:12.2.0.1-slim oracle/database:12.2.0.1 Copy all the above built and pulled images to all the nodes in your cluster or add to a Docker registry that your cluster can access.  NOTE: If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the Docker image available to a registry visible to your Kubernetes cluster. Upload your image to a machine running Docker and Kubernetes as follows:\n# on your build machine $ docker save Image_Name:Tag \u0026gt; Image_Name-Tag.tar $ scp Image_Name-Tag.tar YOUR_USER@YOUR_SERVER:/some/path/Image_Name-Tag.tar # on the Kubernetes server $ docker load \u0026lt; /some/path/Image_Name-Tag.tar Set Up the Code Repository to Deploy Oracle WebCenter Sites Domain Oracle WebCenter Sites domain deployment on Kubernetes leverages the Oracle WebLogic Kubernetes Operator infrastructure. For deploying the Oracle WebCenter Sites domain, you need to set up the deployment scripts as below:\n  Create a working directory to setup the source code.\n$ mkdir \u0026lt;work directory\u0026gt; $ cd \u0026lt;work directory\u0026gt;   Download the supported version of Oracle WebLogic Kubernetes Operator source code archieve file (.zip/.tar.gz) from the operator relases page. Currently the supported operator version can be downloaded from 2.4.0.\n  Extract the source code archive file (.zip/.tar.gz) in to the work directory.\n  Download the WebCenter Sites kubernetes deployment scripts from this repository and copy them in to WebLogic operator samples location.\n$ git clone https://github.com/oracle/fmw-kubernetes.git $ cp -rf \u0026lt;work directory\u0026gt;/fmw-kubernetes/OracleWebCenterSites/kubernetes/2.4.0/create-wcsites-domain \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0/kubernetes/samples/scripts/   You can now use the deployment scripts from \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0 to set up the WebCenter Sites domain as further described in this document.\nThis will be your home directory for runnning all the required scripts.\n$ cd \u0026lt;work directory\u0026gt;/weblogic-kubernetes-operator-2.4.0 Grant Roles and Clear Stale Resources   Grant the Helm service account the cluster-admin role:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF output: clusterrolebinding.rbac.authorization.k8s.io/helm-user-cluster-admin-role configured   To confirm if there is already a WebLogic custom resource definition, execute the following command:\n$ kubectl get crd NAME CREATED AT domains.weblogic.oracle 2020-03-14T12:10:21Z   If you find any WebLogic custom resource definition, then delete it by executing the following command:\n$ kubectl delete crd domains.weblogic.oracle customresourcedefinition.apiextensions.k8s.io \u0026#34;domains.weblogic.oracle\u0026#34; deleted   Install the WebLogic Kubernetes Operator   Create a namespace for the WebLogic Kubernetes Operator:\n$ kubectl create namespace operator-ns namespace/operator-ns created NOTE: For this exercise we are creating a namespace called \u0026ldquo;operator-ns\u0026rdquo; (can be any name).\nYou can also use:\n domainUID/domainname as wcsitesinfra Domain namespace as wcsites-ns Operator namespace as operator-ns traefik namespace as traefik    Create a service account for the WebLogic Kubernetes Operator in the Operator\u0026rsquo;s namespace:\n$ kubectl create serviceaccount -n operator-ns operator-sa serviceaccount/operator-sa created   To be able to set up the log-stash and Elasticsearch after creating the domain, set the value of the field elkIntegrationEnabled to true in the file kubernetes/charts/weblogic-operator/values.yaml.\n  Use helm to install and start the WebLogic Kubernetes Operator from the downloaded repository:\n$ helm install kubernetes/charts/weblogic-operator --name weblogic-kubernetes-operator \\ --namespace operator-ns --set serviceAccount=operator-sa --set \u0026#34;domainNamespaces={}\u0026#34; --wait OUTPUT:\nNAME: weblogic-kubernetes-operator LAST DEPLOYED: Sat Mar 14 12:19:45 2020 NAMESPACE: operator-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ClusterRoleBinding NAME AGE operator-ns-weblogic-operator-clusterrolebinding-nonresource 56s operator-ns-weblogic-operator-clusterrolebinding-discovery 56s operator-ns-weblogic-operator-clusterrolebinding-auth-delegator 56s operator-ns-weblogic-operator-clusterrolebinding-general 56s ==\u0026gt; v1/RoleBinding NAME AGE weblogic-operator-rolebinding-namespace 56s weblogic-operator-rolebinding 56s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE internal-weblogic-operator-svc ClusterIP 10.105.252.222 \u0026lt;none\u0026gt; 8082/TCP 56s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE weblogic-operator 1 1 1 1 56s ==\u0026gt; v1/Secret NAME TYPE DATA AGE weblogic-operator-secrets Opaque 1 56s ==\u0026gt; v1/ConfigMap NAME DATA AGE weblogic-operator-cm 3 56s ==\u0026gt; v1/ClusterRole NAME AGE operator-ns-weblogic-operator-clusterrole-namespace 56s operator-ns-weblogic-operator-clusterrole-general 56s operator-ns-weblogic-operator-clusterrole-nonresource 56s operator-ns-weblogic-operator-clusterrole-operator-admin 56s operator-ns-weblogic-operator-clusterrole-domain-admin 56s ==\u0026gt; v1/Role NAME AGE weblogic-operator-role 56s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 56s ``\n  To verify that the Operator\u0026rsquo;s pod is running, list the pods in the Operator\u0026rsquo;s namespace. You should see one for the WebLogic Kubernetes Operator:\n$ kubectl get pods -n operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 3m15s   Then, check by viewing the Operator pod\u0026rsquo;s log as shown in the following sample log snippet:\n$ kubectl logs -n operator-ns -c weblogic-operator deployments/weblogic-operator Launching Oracle WebLogic Server Kubernetes Operator... Importing keystore /operator/internal-identity/temp/weblogic-operator.jks to /operator/internal-identity/temp/weblogic-operator.p12... Entry for alias weblogic-operator-alias successfully imported. Import command completed: 1 entries successfully imported, 0 entries failed or cancelled Warning: The -srcstorepass option is specified multiple times. All except the last one will be ignored. MAC verified OK % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 4249 0 2394 100 1855 6884 5334 --:--:-- --:--:-- --:--:-- 6899 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5558 0 3028 100 2530 22704 18970 --:--:-- --:--:-- --:--:-- 22766 OpenJDK 64-Bit Server VM warning: Option MaxRAMFraction was deprecated in version 10.0 and will likely be removed in a future release. VM settings: Max. Heap Size (Estimated): 14.08G Using VM: OpenJDK 64-Bit Server VM {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.438+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.TuningParametersImpl\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;update\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593438,\u0026#34;message\u0026#34;:\u0026#34;Reloading tuning parameters from Operator\u0026#39;s config map\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.944+0000\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593944,\u0026#34;message\u0026#34;:\u0026#34;Oracle WebLogic Server Kubernetes Operator, version: 2.4.0, implementation: master.4d4fe0a, build time: 2019-11-15T21:19:56-0500\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:53.972+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168593972,\u0026#34;message\u0026#34;:\u0026#34;Operator namespace is: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.009+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594009,\u0026#34;message\u0026#34;:\u0026#34;Operator target namespaces are: operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.013+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;begin\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594013,\u0026#34;message\u0026#34;:\u0026#34;Operator service account is: operator-sa\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.031+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performK8sVersionCheck\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594031,\u0026#34;message\u0026#34;:\u0026#34;Verifying Kubernetes minimum version\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.286+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ClientPool\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getApiClient\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594286,\u0026#34;message\u0026#34;:\u0026#34;The Kuberenetes Master URL is set to https://10.96.0.1:443\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:54.673+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;createAndValidateKubernetesVersion\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168594673,\u0026#34;message\u0026#34;:\u0026#34;Kubernetes version is: v1.13.7\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.259+0000\u0026#34;,\u0026#34;thread\u0026#34;:12,\u0026#34;fiber\u0026#34;:\u0026#34;engine-operator-thread-2-fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.CrdHelper$CrdContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595259,\u0026#34;message\u0026#34;:\u0026#34;Create Custom Resource Definition: oracle.kubernetes.operator.calls.CallResponse@470b40c\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.356+0000\u0026#34;,\u0026#34;thread\u0026#34;:16,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;performSecurityChecks\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595356,\u0026#34;message\u0026#34;:\u0026#34;Verifying that operator service account can access required operations on required resources in namespace operator-ns\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.598+0000\u0026#34;,\u0026#34;thread\u0026#34;:18,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1-child-2\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.ConfigMapHelper$ScriptConfigMapContext$CreateResponseStep\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;onSuccess\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595598,\u0026#34;message\u0026#34;:\u0026#34;Creating domain config map, operator-ns, for namespace: {1}.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.937+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.utils.Certificates\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;getCertificate\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595937,\u0026#34;message\u0026#34;:\u0026#34;Can\u0026#39;t read certificate at /operator/external-identity/externalOperatorCert\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\\njava.nio.file.NoSuchFileException: /operator/external-identity/externalOperatorCert\\n\\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\\n\\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\\n\\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:215)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:370)\\n\\tat java.base/java.nio.file.Files.newByteChannel(Files.java:421)\\n\\tat java.base/java.nio.file.Files.readAllBytes(Files.java:3205)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getCertificate(Certificates.java:48)\\n\\tat oracle.kubernetes.operator.utils.Certificates.getOperatorExternalCertificateData(Certificates.java:39)\\n\\tat oracle.kubernetes.operator.rest.RestConfigImpl.getOperatorExternalCertificateData(RestConfigImpl.java:52)\\n\\tat oracle.kubernetes.operator.rest.RestServer.isExternalSslConfigured(RestServer.java:383)\\n\\tat oracle.kubernetes.operator.rest.RestServer.start(RestServer.java:199)\\n\\tat oracle.kubernetes.operator.Main.startRestServer(Main.java:353)\\n\\tat oracle.kubernetes.operator.Main.completeBegin(Main.java:198)\\n\\tat oracle.kubernetes.operator.Main$NullCompletionCallback.onCompletion(Main.java:701)\\n\\tat oracle.kubernetes.operator.work.Fiber.completionCheck(Fiber.java:475)\\n\\tat oracle.kubernetes.operator.work.Fiber.run(Fiber.java:448)\\n\\tat oracle.kubernetes.operator.work.ThreadLocalContainerResolver.lambda$wrapExecutor$0(ThreadLocalContainerResolver.java:87)\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:834)\\n\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:55.967+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168595967,\u0026#34;message\u0026#34;:\u0026#34;Did not start the external ssl REST server because external ssl has not been configured.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} {\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.910+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.rest.RestServer\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;start\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597910,\u0026#34;message\u0026#34;:\u0026#34;Started the internal ssl REST server on https://0.0.0.0:8082/operator\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}\t{\u0026#34;timestamp\u0026#34;:\u0026#34;03-14-2020T06:49:57.913+0000\u0026#34;,\u0026#34;thread\u0026#34;:21,\u0026#34;fiber\u0026#34;:\u0026#34;fiber-1\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.Main\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;markReadyAndStartLivenessThread\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584168597913,\u0026#34;message\u0026#34;:\u0026#34;Starting Operator Liveness Thread\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;}   Configure NFS (Network File System) Server To configure NFS server, install the nfs-utils package preferably on Master node:\n$ sudo yum install nfs-utils To start the nfs-server service, and configure the service to start following a system reboot:\n$ sudo systemctl start nfs-server $ sudo systemctl enable nfs-server Create the directory you want to export as the NFS share, for example /scratch/K8SVolume:\n$ sudo mkdir -p /scratch/K8SVolume $ sudo chown -R 1000:1000 /scratch/K8SVolume host name or IP address of the NFS Server\nNote: Host name or IP address of the NFS Server and NFS Share path which is used when you create PV/PVC in further sections.\nPrepare the Environment for the WebCenter Sites Domain   Unless you would like to use the default namespace, create a Kubernetes namespace that can host one or more domains:\n$ kubectl create namespace wcsites-ns namespace/wcsites-ns created   To manage domains in this namespace, configure the Operator using helm:\n$ helm upgrade --reuse-values --set \u0026#34;domainNamespaces={wcsites-ns}\u0026#34; \\  --wait weblogic-kubernetes-operator kubernetes/charts/weblogic-operator Release \u0026#34;weblogic-kubernetes-operator\u0026#34; has been upgraded. Happy Helming! LAST DEPLOYED: Sat Mar 14 12:25:36 2020 NAMESPACE: operator-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE weblogic-operator-67df5fddc5-tlc4b 2/2 Running 0 5m53s ==\u0026gt; v1/ClusterRole NAME AGE operator-ns-weblogic-operator-clusterrole-domain-admin 5m53s operator-ns-weblogic-operator-clusterrole-operator-admin 5m53s operator-ns-weblogic-operator-clusterrole-nonresource 5m53s operator-ns-weblogic-operator-clusterrole-general 5m53s operator-ns-weblogic-operator-clusterrole-namespace 5m53s ==\u0026gt; v1/ClusterRoleBinding NAME AGE operator-ns-weblogic-operator-clusterrolebinding-general 5m53s operator-ns-weblogic-operator-clusterrolebinding-discovery 5m53s operator-ns-weblogic-operator-clusterrolebinding-nonresource 5m53s operator-ns-weblogic-operator-clusterrolebinding-auth-delegator 5m53s ==\u0026gt; v1/RoleBinding NAME AGE weblogic-operator-rolebinding-namespace 3s weblogic-operator-rolebinding 5m53s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE internal-weblogic-operator-svc ClusterIP 10.105.252.222 \u0026lt;none\u0026gt; 8082/TCP 5m53s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE weblogic-operator 1 1 1 1 5m53s ==\u0026gt; v1/Secret NAME TYPE DATA AGE weblogic-operator-secrets Opaque 1 5m53s ==\u0026gt; v1/ConfigMap NAME DATA AGE weblogic-operator-cm 3 5m53s ==\u0026gt; v1/Role NAME AGE weblogic-operator-role 5m53s   Create Kubernetes secrets:\na. Using the create-weblogic-credentials script, create a Kubernetes secret that contains the user name and password for the domain in the same Kubernetes namespace as the domain:\nOutput:\n$ sh kubernetes/samples/scripts/create-weblogic-domain-credentials/create-weblogic-credentials.sh \\  -u weblogic -p Welcome1 -n wcsites-ns \\  -d wcsitesinfra -s wcsitesinfra-domain-credentials secret/wcsitesinfra-domain-credentials created secret/wcsitesinfra-domain-credentials labeled The secret wcsitesinfra-domain-credentials has been successfully created in the wcsites-ns namespace. Where:\n* weblogic is the weblogic username * Welcome1 is the weblogic password * wcsitesinfra is the domain name * wcsites-ns is the domain namespace * wcsitesinfra-domain-credentials is the secret name  Note: You can inspect the credentials as follows:\n$ kubectl get secret wcsitesinfra-domain-credentials -o yaml -n wcsites-ns b. Create a Kubernetes secret for the Repository Configuration Utility (user name and password) using the create-rcu-credentials.sh script in the same Kubernetes namespace as the domain:\nOutput:\n$ sh kubernetes/samples/scripts/create-rcu-credentials/create-rcu-credentials.sh \\  -u WCS1 -p Welcome1 -a sys -q Oradoc_db1 -n wcsites-ns \\  -d wcsitesinfra -s wcsitesinfra-rcu-credentials secret/wcsitesinfra-rcu-credentials created secret/wcsitesinfra-rcu-credentials labeled The secret wcsitesinfra-rcu-credentials has been successfully created in the wcsites-ns namespace. Where:\n* WCS1 is the schema user * Welcome1 is the schema password * Oradoc_db1 is the database SYS users password * wcsitesinfra is the domain name * wcsites-ns is the domain namespace * wcsitesinfra-rcu-credentials is the secret name  Note: You can inspect the credentials as follows:\n$ kubectl get secret wcsitesinfra-rcu-credentials -o yaml -n wcsites-ns   Create a Kubernetes PV and PVC (Persistent Volume and Persistent Volume Claim):\na. Update the kubernetes/samples/scripts/create-wcsites-domain/utils/create-wcsites-pv-pvc-inputs.yaml.\nReplace the token %NFS_SERVER% with the host name/IP of NFS Server created in Configure NFS Server section.\nIn the NFS Server, create a folder and grant permissions as given below:\n$ sudo rm -rf /scratch/K8SVolume/WCSites \u0026amp;\u0026amp; sudo mkdir -p /scratch/K8SVolume/WCSites \u0026amp;\u0026amp; sudo chown 1000:1000 /scratch/K8SVolume/WCSites Update the weblogicDomainStoragePath paramter with /scratch/K8SVolume/WCSites.\nb. Execute the create-pv-pvc.sh script to create the PV and PVC configuration files:\n$ sh kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/create-pv-pvc.sh \\  -i kubernetes/samples/scripts/create-wcsites-domain/utils/create-wcsites-pv-pvc-inputs.yaml \\  -o kubernetes/samples/scripts/create-wcsites-domain/output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-pv-pvc-inputs-v1\u0026#34; export baseName=\u0026#34;domain\u0026#34; export domainUID=\u0026#34;wcsitesinfra\u0026#34; export namespace=\u0026#34;wcsites-ns\u0026#34; export weblogicDomainStorageType=\u0026#34;HOST_PATH\u0026#34; export weblogicDomainStoragePath=\u0026#34;/scratch/K8SVolume/WCSites\u0026#34; export weblogicDomainStorageReclaimPolicy=\u0026#34;Retain\u0026#34; export weblogicDomainStorageSize=\u0026#34;10Gi\u0026#34; Generating kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml Generating kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml The following files were generated: kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml Completed c. To create the PV and PVC, use kubectl create with output configuration files:\nOutput:\n$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pv.yaml \\  -f kubernetes/samples/scripts/create-wcsites-domain/output/pv-pvcs/wcsitesinfra-domain-pvc.yaml persistentvolume/wcsitesinfra-domain-pv created persistentvolumeclaim/wcsitesinfra-domain-pvc created Note: You can verify the PV and PV\u0026rsquo;s details as follows:\n$ kubectl describe pv wcsitesinfra-domain-pv -n wcsites-ns $ kubectl describe pvc wcsitesinfra-domain-pvc -n wcsites-ns   Label the nodes in the Kubernetes cluster for the targeted scheduling of the servers on particular nodes as needed:\nkubectl label node \u0026lt;node-name\u0026gt; name=abc Note: Here \u0026lt;node-name\u0026gt; is the node as displayed in the NAME field of kubectl get nodes command. abc is the label that we are defining. Label is a key, value pair and can be anything meaningful. The same should be used for nodeSelector.\nFor scheduling we can select these nodes based on the labels.\n  Set Up the Database You must set up the database before you create your domain. For testing and development, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nThe Oracle Database Docker images are supported for non-production use only. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).   Database Creation with PV: (Recommended)\nFor testing and development of heavy usage, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nReplace the token %NFS_SERVER% with the host name/IP of NFS Server created in Configure NFS Server section.\nIn the NFS Server, create a folder and grant permissions as given below:\n$ sudo rm -rf /scratch/K8SVolume/WCSitesDB \u0026amp;\u0026amp; sudo mkdir -p /scratch/K8SVolume/WCSitesDB \u0026amp;\u0026amp; sudo chown 54321 /scratch/K8SVolume/WCSitesDB Update the above Persistent Volume created value for the path parameter in kubernetes/samples/scripts/create-wcsites-domain/create-database/db-with-pv.yaml\nCreate a Kubernetes namespace for database.\n$ kubectl create namespace wcsitesdb-ns namespace/wcsitesdb-ns created -bash-4.2$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/create-database/db-with-pv.yaml persistentvolume/oracle-db-pv created persistentvolumeclaim/oracle-db-pvc created service/oracle-db created deployment.extensions/oracle-db created To get the pod details for inspecting the logs (if required).\n-bash-4.2$ kubectl get all -n wcsitesdb-ns NAME READY STATUS RESTARTS AGE pod/oracle-db-7bcd584846-6x5lq 0/1 Running 0 3s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oracle-db LoadBalancer 10.97.14.205 \u0026lt;pending\u0026gt; 1521:30011/TCP 44s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oracle-db 0/1 1 0 3s NAME DESIRED CURRENT READY AGE replicaset.apps/oracle-db-7bcd584846 1 1 0 3s For checking database logs:\n-bash-4.2$ kubectl logs -f -n wcsitesdb-ns oracle-db-7bcd584846-6x5lq Check for value Done ! The database is ready for use . to confirm if database is started successfully.\nNow, for creating a Fusion Middleware domain, you can use the database connection string, oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s, as an rcuDatabaseURL parameter in the domain.input.yaml file.\n  Database Creation without PV:\nFor quick testing and development of normal usage, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nCreate a Kubernetes namespace for database:\n$ kubectl create namespace wcsitesdb-ns namespace/wcsitesdb-ns created Check the help command to see the input parameters:\n$ sh kubernetes/samples/scripts/create-oracle-db-service/start-db-service.sh -h usage: kubernetes/samples/scripts/create-oracle-db-service/start-db-service.sh -p \u0026lt;nodeport\u0026gt; -i \u0026lt;image\u0026gt; -s \u0026lt;pullsecret\u0026gt; -n \u0026lt;namespace\u0026gt; [-h] -i Oracle DB Image (optional) (default: container-registry.oracle.com/database/enterprise:12.2.0.1-slim ) -p DB Service NodePort (optional) (default: 30011) -s DB Image PullSecret (optional) (default: docker-store) -n Configurable Kubernetes NameSpace for Oracle DB Service (optional) (default: default) -h Help To create and start the database, run the below command and then monitor the status till the database is ready for use:\n-bash-4.2$ sh kubernetes/samples/scripts/create-oracle-db-service/start-db-service.sh -n wcsitesdb-ns Checking Status for NameSpace [wcsitesdb-ns] Error from server (NotFound): namespaces \u0026#34;wcsitesdb-ns\u0026#34; not found Adding NameSpace[wcsitesdb-ns] to Kubernetes Cluster namespace/wcsitesdb-ns created NodePort[30011] ImagePullSecret[docker-store] Image[container-registry.oracle.com/database/enterprise:12.2.0.1-slim] NameSpace[wcsitesdb-ns] service/oracle-db created deployment.extensions/oracle-db created service/oracle-db unchanged deployment.extensions/oracle-db unchanged [oracle-db-99df9b6c9-rpz5l] already initialized .. Checking Pod READY column for State [1/1] Pod [oracle-db-99df9b6c9-rpz5l] Status is Ready Iter [1/60] NAME READY STATUS RESTARTS AGE oracle-db-99df9b6c9-rpz5l 1/1 Running 0 6s NAME READY STATUS RESTARTS AGE oracle-db-99df9b6c9-rpz5l 1/1 Running 0 6s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oracle-db LoadBalancer 10.109.254.7 \u0026lt;pending\u0026gt; 1521:30011/TCP 6s [1/20] Retrying for Oracle Database Availability... [2/20] Retrying for Oracle Database Availability... ... [9/20] Retrying for Oracle Database Availability... [10/20] Retrying for Oracle Database Availability... Done ! The database is ready for use . Oracle DB Service is RUNNING with NodePort [30011] Oracle DB Service URL [oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s] To get the pod details for inspecting the logs if required:\n-bash-4.2$ kubectl get all -n wcsitesdb-ns NAME READY STATUS RESTARTS AGE pod/oracle-db-99df9b6c9-rpz5l 1/1 Running 0 3m6s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oracle-db LoadBalancer 10.109.254.7 \u0026lt;pending\u0026gt; 1521:30011/TCP 3m6s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/oracle-db 1/1 1 1 3m6s NAME DESIRED CURRENT READY AGE replicaset.apps/oracle-db-99df9b6c9 1 1 1 3m6s Now, for creating a Fusion Middleware domain, you can use the database connection string, oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s, as an rcuDatabaseURL parameter in the domain.input.yaml file.\n  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/create-soa-domains/",
	"title": "Create SOA domains",
	"tags": [],
	"description": "Sample for creating a SOA Suite domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated SOA domain.",
	"content": " Oracle SOA Suite is currently supported only for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle SOA Suite in Kubernetes before it is supported for production use.\n The SOA deployment scripts demonstrate the creation of a SOA Suite domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n Review the Domain resource documentation. Review the complete information available in prerequisites document. Ensure that you have executed all the preliminary steps documented in prepare-your-environment. Ensure that the database and the Weblogic Kubernetes operator is up.  Prepare to use the create domain script The sample scripts for Oracle SOA Suite domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-soa-domain.\nYou must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Please refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is soa_cluster for the SOA domain. You can update this to osb_cluster for an OSB domain type or soa_cluster for SOAESS or SOAOSB or SOAESSOSB domain types. soa_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the SOA domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/soainfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. soainfra   domainType Type of the domain. Mandatory input for SOA Suite domains. You must provide one of the supported domain type values: soa (deploys a SOA domain),osb (deploys an OSB (Oracle Service Bus) domain),soaess (deploys a SOA domain with Enterprise Scheduler (ESS)),soaosb (deploys a domain with SOA and OSB), and soaessosb (deploys a domain with SOA, OSB, and ESS). soa   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image SOA Suite Docker image. The operator requires SOA Suite 12.2.1.4. Refer to SOA domains for details on how to obtain or create the image. container-registry.oracle.com/middleware/soasuite:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/soainfra   managedServerNameBase Base string used to generate Managed Server names. soa_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. soans   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. soainfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. soainfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example SOA1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. SOA1   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes secret containing the database credentials. soainfra-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a SOA Suite domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes job that will start up a utility SOA Suite container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named soa_cluster-1 of size 5. Two Managed Servers, named soa_server1 and soa_server2, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. SOA Infra, SOA composer and WorklistApp applications deployed. No data sources or JMS resources. A T3 channel.  Verify the results The create domain script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n$ cat output/weblogic-domains/soainfra/domain.yaml # Copyright (c) 2017, 2019, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v6\u0026quot; kind: Domain metadata: name: soainfra namespace: soans labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: soainfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/soainfra # If the domain home is in the image domainHomeInImage: false # The WebLogic Server Docker image that the Operator uses to start the domain image: \u0026quot;container-registry.oracle.com/middleware/soasuite:12.2.1.4\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: soainfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # The in-pod location for domain log, server logs, server out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/soainfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverService: precreateService: true serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: soainfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: osb_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: soa_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain soainfra -n soans Name: soainfra Namespace: soans Labels: weblogic.domainUID=soainfra weblogic.resourceVersion=domain-v2 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v6 Kind: Domain Metadata: Creation Timestamp: 2020-01-27T10:04:11Z Generation: 6 Resource Version: 18537800 Self Link: /apis/weblogic.oracle/v6/namespaces/soans/domains/soainfra UID: 5dcb76e4-40ec-11ea-b332-020017041cc2 Spec: Admin Server: Admin Service: Annotations: Channels: Labels: Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: osb_cluster Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Volumes: Server Service: Annotations: Labels: Precreate Service: true Server Start State: RUNNING Cluster Name: soa_cluster Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Volumes: Server Service: Annotations: Labels: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/soainfra Domain Home In Image: false Image: container-registry.oracle.com/middleware/soasuite:12.2.1.4. Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/soainfra Log Home Enabled: true Managed Servers: Server Pod: Annotations: Container Security Context: Containers: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Mount Path: /u01/oracle/user_projects Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: soainfra-domain-pvc Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: soainfra-domain-credentials Status: Clusters: Cluster Name: soa_cluster Maximum Replicas: 5 Cluster Name: osb_cluster Maximum Replicas: 5 Conditions: Servers: Health: Activation Time: 2020-01-27T10:08:18.876Z Overall Health: ok Subsystems: Node Name: MyNode Server Name: AdminServer State: RUNNING Start Time: 2020-01-27T10:04:11.853Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and two Managed Servers for each cluster (SOA and OSB) are running for soaessosb domain type.\n$ kubectl get pods -n soans NAME READY STATUS RESTARTS AGE soainfra-adminserver 1/1 Running 0 20h soainfra-osb-server1 1/1 Running 0 20h soainfra-osb-server2 1/1 Running 0 20h soainfra-soa-server1 1/1 Running 0 20h soainfra-soa-server2 1/1 Running 0 20h Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command. You can verify that services for Administration Server and Managed Servers (for SOA and OSB clusters) are created for soaessosb domain type.\n$ kubectl get services -n soans NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE soainfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 20h soainfra-cluster-osb-cluster ClusterIP 10.110.6.107 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-cluster-soa-cluster ClusterIP 10.100.165.105 \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-osb-server1 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server2 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server3 ClusterIP 10.99.1.111 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server4 ClusterIP 10.106.178.175 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server5 ClusterIP 10.97.65.163 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server3 ClusterIP 10.104.189.192 \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server4 ClusterIP 10.100.168.31 \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server5 ClusterIP 10.101.171.78 \u0026lt;none\u0026gt; 8001/TCP 20h "
},
{
	"uri": "/fmw-kubernetes/soa-domains/manage-soa-domains/delete-domain-home/",
	"title": "Delete the SOA domain home",
	"tags": [],
	"description": "Learn about the steps to cleanup the SOA domain home.",
	"content": "Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to weblogic-operator-output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/create-wcsites-domains/",
	"title": "Create WebCenter Sites domains",
	"tags": [],
	"description": "Sample for creating a WebCenter Sites domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebCenter Sites domain.",
	"content": " Oracle WebCenter Sites is currently supported for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle WebCenter Sites in Kubernetes before it is supported for production use.\n Contents  Introduction Prerequisites Prepare the WebCenter Sites Domain Creation Input File Create the WebCenter Sites Domain Initialize the WebCenter Sites Domain Verify the WebCenter Sites Domain Expose WebCenter Sites Services Load Balance With an Ingress Controller or A Web Server Configure WebCenter Sites Settings in WebCenter Sites Property Management For Publishing Setting in WebCenter Sites  Introduction This document details on how to use sample scripts to demonstrate the creation of a WebCenter Sites domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites  Ensure that you have completed all of the steps under prepare-your-environment. Ensure that the database and the WebLogic Kubernetes operator is up.  Prepare the WebCenter Sites Domain Creation Input File If required, domain creation inputs can be customized by editing create-domain-inputs.yaml as described below:\nPlease note that the sample scripts for the WebCenter Sites domain deployment are available from the previously downloaded repository at kubernetes/samples/scripts/create-wcsites-domain/domain-home-on-pv/.\nMake a copy of the create-domain-inputs.yaml file before updating the default values.\nThe default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named wcsites_cluster of size 5. Managed Server, named wcsites_server1, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;.  Configuration parameters The following parameters can be provided in the inputs file:\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is wcsites_cluster for the WebCenter Sites domain. wcsites_cluster   configuredManagedServerCount Number of Managed Server instances for the domain. 3   createDomainFilesDir Directory on the host machine to locate all the files that you need to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebCenter Sites domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/wcsitesinfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects/domains   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. wcsitesinfra   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebCenter Sites Docker image. The Operator requires WebCenter Sites release 12.2.1.4.0. Refer to WebCenter Sites Docker Image for details on how to obtain or create the image. oracle/wcsites:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server.out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Server to initially start for the domain. 1   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can include references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/wcsitesinfra   managedServerNameBase Base string used to generate Managed Server names. wcsites_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. wcsites-ns   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. wcsitesinfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. wcsites-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed and minimum amount of compute resources required for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified. Refer to WebCenter Sites Cluster Sizing Recommendations for more details.   rcuSchemaPrefix The schema prefix to use in the database, for example WCS1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. WCS1   rcuDatabaseURL The database URL. oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The loadbalancer hostname to be provided. wcsites-rcu-credentials   loadBalancerHostName Hostname for the final url accessible outside K8S environment. abc.def.com   loadBalancerPortNumber Port for the final url accessible outside K8S environment. 30305   loadBalancerProtocol Protocol for the final url accessible outside K8S environment. http   loadBalancerType Loadbalancer name that will be used. Example: Traefik or \u0026quot;\u0026rdquo; traefik   unicastPort Starting range of uniciast port that application will use. 50000   sitesSamples Sites to be installed without samples sites by default, else true. false    You can form the names of the Kubernetes resources in the generated YAML files with the value of these properties specified in the create-domain-inputs.yaml file: adminServerName , clusterName and managedServerNameBase . Characters that are invalid in a Kubernetes service name are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;) .\nThe sample demonstrates how to create a WebCenter Sites domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. You can modify the generated domain YAML file to include more use cases.\nCreate the WebCenter Sites Domain   Understanding the syntax of the create-domain.sh script:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script performs the following functions:\n  Creates a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, remove its content before using this script.\n  Creates a Kubernetes job that will start up a utility WebCenter Sites container and run offline WLST scripts to create the domain on the shared storage.\n  Runs and waits for the job to finish.\n  Creates a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f ../\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Creates a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n    Now, run the create-domain.sh sample script below, pointing it at the create-domain-inputs inputs file and an output directory like below:\nbash-4.2$ rm -rf kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains bash-4.2$ sh kubernetes/samples/scripts/create-wcsites-domain/domain-home-on-pv/create-domain.sh \\  -i kubernetes/samples/scripts/create-wcsites-domain/domain-home-on-pv/create-domain-inputs.yaml \\  -o kubernetes/samples/scripts/create-wcsites-domain/output Input parameters being used export version=\u0026#34;create-weblogic-sample-domain-inputs-v1\u0026#34; export adminPort=\u0026#34;7001\u0026#34; export adminServerName=\u0026#34;adminserver\u0026#34; export domainUID=\u0026#34;wcsitesinfra\u0026#34; export domainHome=\u0026#34;/u01/oracle/user_projects/domains/$domainUID\u0026#34; export serverStartPolicy=\u0026#34;IF_NEEDED\u0026#34; export clusterName=\u0026#34;wcsites_cluster\u0026#34; export configuredManagedServerCount=\u0026#34;3\u0026#34; export initialManagedServerReplicas=\u0026#34;1\u0026#34; export managedServerNameBase=\u0026#34;wcsites_server\u0026#34; export managedServerPort=\u0026#34;8001\u0026#34; export image=\u0026#34;oracle/wcsites:12.2.1.4\u0026#34; export imagePullPolicy=\u0026#34;IfNotPresent\u0026#34; export productionModeEnabled=\u0026#34;true\u0026#34; export weblogicCredentialsSecretName=\u0026#34;wcsitesinfra-domain-credentials\u0026#34; export includeServerOutInPodLog=\u0026#34;true\u0026#34; export logHome=\u0026#34;/u01/oracle/user_projects/domains/logs/$domainUID\u0026#34; export t3ChannelPort=\u0026#34;30012\u0026#34; export exposeAdminT3Channel=\u0026#34;false\u0026#34; export adminNodePort=\u0026#34;30701\u0026#34; export exposeAdminNodePort=\u0026#34;false\u0026#34; export namespace=\u0026#34;wcsites-ns\u0026#34; javaOptions=-Dweblogic.StdoutDebugEnabled=false -Xms2g export persistentVolumeClaimName=\u0026#34;wcsitesinfra-domain-pvc\u0026#34; export domainPVMountPath=\u0026#34;/u01/oracle/user_projects/domains\u0026#34; export createDomainScriptsMountPath=\u0026#34;/u01/weblogic\u0026#34; export createDomainScriptName=\u0026#34;create-domain-job.sh\u0026#34; export createDomainFilesDir=\u0026#34;wlst\u0026#34; export rcuSchemaPrefix=\u0026#34;WCS1\u0026#34; export rcuDatabaseURL=\u0026#34;oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s\u0026#34; export rcuCredentialsSecret=\u0026#34;wcsitesinfra-rcu-credentials\u0026#34; export loadBalancerHostName=\u0026#34;abc.def.com\u0026#34; export loadBalancerPortNumber=\u0026#34;30305\u0026#34; export loadBalancerProtocol=\u0026#34;http\u0026#34; export loadBalancerType=\u0026#34;traefik\u0026#34; export unicastPort=\u0026#34;50000\u0026#34; export sitesSamples=\u0026#34;true\u0026#34; Generating kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml Generating kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/delete-domain-job.yaml Generating kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml Checking to see if the secret wcsitesinfra-domain-credentials exists in namespace wcsites-ns configmap/wcsitesinfra-create-fmw-infra-sample-domain-job-cm created Checking the configmap wcsitesinfra-create-fmw-infra-sample-domain-job-cm was created configmap/wcsitesinfra-create-fmw-infra-sample-domain-job-cm labeled Checking if object type job with name wcsitesinfra-create-fmw-infra-sample-domain-job exists No resources found. $loadBalancerType is NOT empty Creating the domain by creating the job kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml job.batch/wcsitesinfra-create-fmw-infra-sample-domain-job created Waiting for the job to complete... status on iteration 1 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 2 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 3 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 4 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 5 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 6 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 7 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 8 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 9 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 10 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 11 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 12 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 13 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 14 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 15 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Running status on iteration 16 of 20 pod wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh status is Completed Domain wcsitesinfra was created and will be started by the WebLogic Kubernetes Operator The following files were generated: kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-inputs.yaml kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/create-domain-job.yaml kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml Completed\t  To monitor the above domain creation logs:\n$ kubectl get pods -n wcsites-ns |grep wcsitesinfra-create wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 1/1 Running 0 6s $ kubectl get pods -n wcsites-ns | grep wcsitesinfra-create | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl -n wcsites-ns logs -f SAMPLE OUTPUT:\nThe domain will be created using the script /u01/weblogic/createSitesDomain.sh Install Automation -\u0026gt; Starting automation script [mkdir] Created dir: /u01/wcs-wls-docker-install/work [echo] [3/14/20 7:54 AM] Work Directory=/u01/wcs-wls-docker-install/work [echo] [3/14/20 7:54 AM] DB URL: jdbc:oracle:thin:@ [echo] [3/14/20 7:54 AM] Info -\u0026gt; The script.db.connectstring has been set. [echo] [3/14/20 7:54 AM] Info.setDBConnectStringPropertey -\u0026gt; setting oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] [3/14/20 7:54 AM] Validation -\u0026gt; Checking if full path to JAVA executable is correctly specified [exec] java version \u0026quot;1.8.0_241\u0026quot; [exec] Java(TM) SE Runtime Environment (build 1.8.0_241-b07) [exec] Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) [echo] [3/14/20 7:54 AM] Validation -\u0026gt; Checking database connection [echo] [3/14/20 7:54 AM] dbUrl-----------------: jdbc:oracle:thin:@oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] [3/14/20 7:54 AM] Database Connection --\u0026gt; Success! [echo] [3/14/20 7:54 AM] 1st phase: WebCenter Sites installation started... [copy] Copying 1 file to /u01/wcs-wls-docker-install/work [copy] Copying /u01/wcs-wls-docker-install/rcu.rsp to /u01/wcs-wls-docker-install/work/rcu.rsp [echo] [3/14/20 7:54 AM] 1st phase: WebCenter Sites installation completed [echo] [3/14/20 7:54 AM] 2nd phase: WebCenter Sites RCU configuration started... [echo] [3/14/20 7:54 AM] Installation -\u0026gt; Repository Creation Utility - creates schema [echo] [3/14/20 7:54 AM] connectString-----------------: oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [replace] Replaced 1 occurrences in 1 files. [echo] [3/14/20 7:54 AM] Create schema using command: /u01/oracle/oracle_common/bin/rcu -silent -responseFile /u01/wcs-wls-docker-install/work/rcu.rsp -f \u0026lt; /u01/wcs-wls-docker-install/work/rcuPasswords8852085298596415722.txt \u0026gt;/u01/wcs-wls-docker-install/work/rcu_output.log [echo] [3/14/20 7:54 AM] RCU Create Schema -\u0026gt; Please wait ... may take several minutes [echo] [3/14/20 8:00 AM] [echo] RCU Logfile: /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/rcu.log [echo] Processing command line .... [echo] Repository Creation Utility - Checking Prerequisites [echo] Checking Global Prerequisites [echo] Repository Creation Utility - Checking Prerequisites [echo] Checking Component Prerequisites [echo] Repository Creation Utility - Creating Tablespaces [echo] Validating and Creating Tablespaces [echo] Create tablespaces in the repository database [echo] Repository Creation Utility - Create [echo] Repository Create in progress. [echo] Executing pre create operations [echo] Percent Complete: 20 [echo] Percent Complete: 20 [echo] Percent Complete: 22 [echo] Percent Complete: 24 [echo] Percent Complete: 26 [echo] Percent Complete: 26 [echo] Percent Complete: 28 [echo] Percent Complete: 28 [echo] Creating Common Infrastructure Services(STB) [echo] Percent Complete: 36 [echo] Percent Complete: 36 [echo] Percent Complete: 46 [echo] Percent Complete: 46 [echo] Percent Complete: 46 [echo] Creating Audit Services Append(IAU_APPEND) [echo] Percent Complete: 54 [echo] Percent Complete: 54 [echo] Percent Complete: 64 [echo] Percent Complete: 64 [echo] Percent Complete: 64 [echo] Creating Audit Services Viewer(IAU_VIEWER) [echo] Percent Complete: 72 [echo] Percent Complete: 72 [echo] Percent Complete: 72 [echo] Percent Complete: 73 [echo] Percent Complete: 73 [echo] Percent Complete: 74 [echo] Percent Complete: 74 [echo] Percent Complete: 74 [echo] Creating Weblogic Services(WLS) [echo] Percent Complete: 79 [echo] Percent Complete: 79 [echo] Percent Complete: 83 [echo] Percent Complete: 83 [echo] Percent Complete: 92 [echo] Percent Complete: 99 [echo] Percent Complete: 99 [echo] Creating Audit Services(IAU) [echo] Percent Complete: 100 [echo] Creating Oracle Platform Security Services(OPSS) [echo] Creating WebCenter Sites(WCSITES) [echo] Executing post create operations [echo] Repository Creation Utility: Create - Completion Summary [echo] Database details: [echo] ----------------------------- [echo] Host Name : oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s [echo] Port : 1521 [echo] Service Name : devpdb.k8s [echo] Connected As : sys [echo] Prefix for (prefixable) Schema Owners : WCS1 [echo] RCU Logfile : /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/rcu.log [echo] Component schemas created: [echo] ----------------------------- [echo] Component Status Logfile [echo] Common Infrastructure Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/stb.log [echo] Oracle Platform Security Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/opss.log [echo] WebCenter Sites Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/wcsites.log [echo] Audit Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau.log [echo] Audit Services Append Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau_append.log [echo] Audit Services Viewer Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/iau_viewer.log [echo] WebLogic Services Success /u01/wcs-wls-docker-install/work/rcu/RCU2020-03-14_07-54_2112542638/logs/wls.log [echo] Repository Creation Utility - Create : Operation Completed [echo] [3/14/20 8:00 AM] Successfully created schemas [echo] [3/14/20 8:00 AM] 2nd phase: WebCenter Sites RCU configuration completed successfully. [echo] [3/14/20 8:00 AM] Oracle WebCenter Sites Installation complete. You can connect to the WebCenter Sites instance at http://10.244.0.252:7002/sites/ Sites RCU Phase completed successfull!!! Sites Installation completed in 366 seconds. --------------------------------------------------------- The domain will be created using the script /u01/weblogic/create-domain-script.sh wlst.sh -skipWLSModuleScanning /u01/weblogic/createSitesDomain.py -oh /u01/oracle -jh /u01/jdk -parent /u01/oracle/user_projects/domains/wcsitesinfra/.. -name wcsitesinfra -user weblogic -password Welcome1 -rcuDb oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s -rcuPrefix WCS1 -rcuSchemaPwd Welcome1 -adminListenPort 7001 -adminName adminserver -managedNameBase wcsites_server -managedServerPort 8001 -prodMode true -managedServerCount 3 -clusterName wcsites_cluster -exposeAdminT3Channel false -t3ChannelPublicAddress 10.123.152.96 -t3ChannelPort 30012 -domainType wcsites -machineName wcsites_machine Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands Creating Admin Server... Creating cluster... Creating Node Managers... managed server name is wcsites_server1 managed server name is wcsites_server2 managed server name is wcsites_server3 ['wcsites_server1', 'wcsites_server2', 'wcsites_server3'] Will create Base domain at /u01/oracle/user_projects/domains/wcsitesinfra Writing base domain... Base domain created at /u01/oracle/user_projects/domains/wcsitesinfra Extending domain at /u01/oracle/user_projects/domains/wcsitesinfra Database oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s ExposeAdminT3Channel false with 10.123.152.96:30012 Applying JRF templates... Extension Templates added Applying WCSITES templates... Extension Templates added Configuring the Service Table DataSource... fmwDb...jdbc:oracle:thin:@oracle-db.wcsitesdb-ns.svc.cluster.local:1521/devpdb.k8s Set user...WCS140_OPSS Set user...WCS140_IAU_APPEND Set user...WCS140_IAU_VIEWER Set user...WCS140_STB Set user...WCS140_WCSITES Getting Database Defaults... Targeting Server Groups... Targeting Server Groups... Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server1 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server2 Set CoherenceClusterSystemResource to defaultCoherenceCluster for server:wcsites_server3 Targeting Cluster ... Set CoherenceClusterSystemResource to defaultCoherenceCluster for cluster:wcsites_cluster Set WLS clusters as target of defaultCoherenceCluster:[wcsites_cluster] Preparing to update domain... Mar 14, 2020 8:01:52 AM oracle.security.jps.az.internal.runtime.policy.AbstractPolicyImpl initializeReadStore INFO: Property for read store in parallel: oracle.security.jps.az.runtime.readstore.threads = null Domain updated successfully Copying /u01/weblogic/server-config-update.sh to PV /u01/oracle/user_projects/domains/wcsitesinfra Copying /u01/weblogic/unicast.py to PV /u01/oracle/user_projects/domains/wcsitesinfra replacing tokens in /u01/oracle/user_projects/domains/wcsitesinfra/server-config-update.sh Successfully Completed   Initialize the WebCenter Sites Domain To start the domain, apply the above domain.yaml:\n$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/output/weblogic-domains/wcsitesinfra/domain.yaml domain.weblogic.oracle/wcsitesinfra created Verify the WebCenter Sites Domain Verify that the domain and servers pods and services are created and in the READY state:\nSample run below:\n-bash-4.2$ kubectl get pods -n wcsites-ns -w NAME READY STATUS RESTARTS\tAGE wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 15m wcsitesinfra-introspect-domain-job-7tvdt 1/1 Running 0 15s wcsitesinfra-introspect-domain-job-7tvdt 0/1 Completed 0 25s wcsitesinfra-introspect-domain-job-7tvdt 0/1 Terminating 0 5s wcsitesinfra-adminserver 0/1 Pending 0 0s wcsitesinfra-adminserver 0/1 Init:0/1 0 0s wcsitesinfra-adminserver 0/1 PodInitializing 0 12s wcsitesinfra-adminserver 0/1 Running 0 13s wcsitesinfra-adminserver 1/1 Running 0 108s wcsitesinfra-wcsites-server1 0/1 Pending 0 0s wcsitesinfra-wcsites-server1 0/1 Init:0/1 0 1s wcsitesinfra-wcsites-server1 0/1 PodInitializing 0 13s wcsitesinfra-wcsites-server1 0/1 Running 0 14s wcsitesinfra-wcsites-server1 1/1 Running 0 96s -bash-4.2$ kubectl get all -n wcsites-ns NAME READY STATUS RESTARTS AGE pod/wcsitesinfra-adminserver 1/1 Running 0 7m5s pod/wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 22m pod/wcsitesinfra-wcsites-server1 1/1 Running 0 5m17s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/wcsitesinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 7m5s service/wcsitesinfra-cluster-wcsites-cluster ClusterIP 10.109.210.3 \u0026lt;none\u0026gt; 8001/TCP 5m17s service/wcsitesinfra-wcsites-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 5m17s NAME COMPLETIONS DURATION AGE job.batch/wcsitesinfra-create-fmw-infra-sample-domain-job 1/1 7m40s 22m To see the Admin and Managed Servers logs, you can check the pod logs:\n$ kubectl logs -f wcsitesinfra-adminserver -n wcsites-ns $ kubectl exec -it wcsitesinfra-adminserver -n wcsites-ns -- /bin/bash $ kubectl logs -f wcsitesinfra-wcsites-server1 -n wcsites-ns $ kubectl exec -it wcsitesinfra-wcsites-server1 -n wcsites-ns -- /bin/bash Verify the Pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get pods -n wcsites-ns NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 56m wcsitesinfra-create-fmw-infra-sample-domain-job-rq4xv 0/1 Completed 0 65m wcsitesinfra-wcsites-server1 1/1 Running 0 41m wcsitesinfra-wcsites-server2 1/1 Running 0 41m Verify the Services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcsites-ns NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 7m38s wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 23m wcsitesinfra-wcsites-server1 1/1 Running 0 5m50s Expose WebCenter Sites Services Below are the default values for exposing services required for all the WebCenter Sites Managed Servers. Reset them if any values are modified.\nDetails on kubernetes/samples/scripts/create-wcsites-domain/utils/wcs-services.yaml:\n name: wcsitesinfra-wcsites-server1-mcp namespace: wcsites-ns weblogic.domainUID: wcsitesinfra weblogic.serverName: wcsites_server1  Execute the below command for exposing the services: (If domain is configured for more than 3 Managed Servers then add the service yaml for additional servers.)\n$ kubectl apply -f kubernetes/samples/scripts/create-wcsites-domain/utils/wcs-services.yaml service/wcsitesinfra-wcsites-server1-np created service/wcsitesinfra-wcsites-server1-svc created service/wcsitesinfra-wcsites-server2-svc created service/wcsitesinfra-wcsites-server3-svc created To verify the services created, here is an example of the output of this command:\n-bash-4.2$ kubectl get services -n wcsites-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 11m wcsitesinfra-cluster-wcsites-cluster ClusterIP 10.109.210.3 \u0026lt;none\u0026gt; 8001/TCP 9m14s wcsitesinfra-wcsites-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 9m14s wcsitesinfra-wcsites-server1-np NodePort 10.105.167.205 \u0026lt;none\u0026gt; 8001:30155/TCP 2m47s wcsitesinfra-wcsites-server1-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s wcsitesinfra-wcsites-server2-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s wcsitesinfra-wcsites-server3-svc ClusterIP None \u0026lt;none\u0026gt; 50000/TCP,50001/TCP,50002/TCP,50003/TCP,50004/TCP,50005/TCP,50006/TCP,50007/TCP,50008/TCP,50009/TCP 2m47s Load Balance With an Ingress Controller or A Web Server You can choose a load balancer provider for your WebLogic domains running in a Kubernetes cluster. Please refer to the WebLogic Kubernetes Operator Load Balancer Samples for information about the current capabilities and setup instructions for each of the supported load balancers.\nFor information on how to set up Loadbalancer for setting up WebCenter Sites domain on K8S:\nFor Traefik, see Setting Up Loadbalancer Traefik for the WebCenter Sites Domain on K8S\nFor Voyager, see Setting Up Loadbalancer Voyager for the WebCenter Sites Domain on K8S\nConfigure WebCenter Sites   Configure WebCenter Sites by hitting url http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/sitesconfigsetup\nWhen installing, select sample sites to be installed and enter the required passwords. Do not change the sites-config location. If you change the location, installation will fail.\n  After the configuration is complete, edit the domain, and restart the Managed Server.\n  To stop Managed Servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 0 }]\u0026#39; To start all configured Managed Servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/clusters/0/replicas\u0026#34;, \u0026#34;value\u0026#34;: 3 }]\u0026#39; Wait till the Managed Server pod is killed and then restart it. Monitor with below command: -bash-4.2$ kubectl get pods -n wcsites-ns -w NAME READY STATUS RESTARTS AGE wcsitesinfra-adminserver 1/1 Running 0 111m wcsitesinfra-create-fmw-infra-sample-domain-job-6l7zh 0/1 Completed 0 126m wcsitesinfra-wcsites-server1 1/1 Running 0 3m7s wcsitesinfra-wcsites-server2 1/1 Running 0 3m7s wcsitesinfra-wcsites-server3 1/1 Running 0 3m7s   Settings in WebCenter Sites Property Management Incase of Voyager Load Balancer: Use Property Management Tool and update cookieserver.validnames property with value JSESSIONID,SERVERID.\nIncase of Traefik Load Balancer: Use Property Management Tool and update cookieserver.validnames property with value JSESSIONID,sticky.\nFor Publishing Setting in WebCenter Sites While configuring publishing destination use NodePort port of target cluster which can be found by executing below command:\n(In this example for publishihng the port 30155 has to be used.)\n-bash-4.2$ kubectl get service/wcsitesinfra-wcsites-server1-np -n wcsites-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-wcsites-server1-np NodePort 10.105.167.205 \u0026lt;none\u0026gt; 8001:30155/TCP 32h "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/",
	"title": "Manage WebCenter Sites domains",
	"tags": [],
	"description": "Sample for managing a WebCenter Sites domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebCenter Sites domain.",
	"content": "Contents  Introduction Integrate Logstash, Elasticsearch and Kibana Set Up WebLogic Logging Exporter Set Up WebLogic Monitoring Exporter Delete the Generated Domain Home Clean Up the create-domain-job Script After Execution Failure  Introduction This document provides instructions to delete or clear an environment in case of errors and steps to integrate with some of the common utility tools to manage the domains created.\nIntegrate Logstash, Elasticsearch and Kibana You can send the operator logs to Elasticsearch, to be displayed in Kibana. Use this sample script to configure Elasticsearch and Kibana deployments and services. For sample configurations on WebCenter Sites, see Elasticsearch integration for the WebLogic Kubernetes Operator\nSet Up WebLogic Logging Exporter After the WebCenter Sites domain is set up, you can publish WebLogic Kubernetes Operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Follow the steps described in this document to set up the Weblogic Logging Exporter and publish the logs to Elasticsearch.\nSet Up WebLogic Monitoring Exporter The WebCenter Sites instance can be monitored using Prometheus and Grafana. The WebLogic Monitoring Exporter uses the WebLogic Server RESTful Management API to scrape runtime information and then exports Prometheus-compatible metrics. It is deployed as a web application in a WebLogic Server (WLS) instance, version 12.2.1 or later, typically, in the instance from which you want to get metrics. For information, see Set Up WebLogic Monitoring Exporter.\nDelete the Generated Domain Home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to weblogic-operator-output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml Clean Up the create-domain-job script After Execution Failure To clean up the create-domain-job script:\n  Get the create domain job and configmaps:\n$ kubectl get configmaps,jobs -n wcsites-ns |grep \u0026#34;create-domain-job\u0026#34;   Delete the job and configmap:\n$ kubectl delete job wcsitesinfra-create-fmw-infra-sample-domain-job -n wcsites-ns $ kubectl delete configmap wcsitesinfra-create-fmw-infra-sample-domain-job-cm -n wcsites-ns   Delete the contents of the PV, if any:\n$ sudo rm -rf /scratch/K8SVolume/WCSites   "
},
{
	"uri": "/fmw-kubernetes/soa-domains/manage-soa-domains/",
	"title": "Manage SOA Domains",
	"tags": [],
	"description": "This document provides steps to integrate with some of the common utility tools to manage the Oralce SOA Suite domains and also the instructions to clean up the environment in case of set up errors.",
	"content": "Important considerations for Oracle SOA Suite domains in Kubernetes.\n Configuring a load balancer for SOA Domains  Learn about configuring an Ingress based load balancer for SOA domains.\n Monitoring a SOA domain  Describes the steps for Monitoring the SOA domain and Publising the logs to Elasticsearch.\n Delete the SOA domain home  Learn about the steps to cleanup the SOA domain home.\n Deploying SOA Composites from Oracle JDeveloper  Understand the steps for deploying SOA Composite applications from Oracle JDeveloper to Oracle SOA in WebLogic Kubernetes Operator Environment.\n Expose T3 protocol for Managed Servers  Learn how to create a T3 channel and the corresponding Kubernetes Service to expose the T3 protocol for Managed Servers in SOA Domain.\n Persisting SOA Adapters Customizations  The lifetime for any customization done in a file on a server pod is upto the lifetime of that pod, the changes are not persisted once the pod goes down or restarted. This document describes the steps for persisting the customizations done for SOA Adapters.\n "
},
{
	"uri": "/fmw-kubernetes/soa-domains/manage-soa-domains/supportjdev/",
	"title": "Deploying SOA Composites from Oracle JDeveloper",
	"tags": [],
	"description": "Understand the steps for deploying SOA Composite applications from Oracle JDeveloper to Oracle SOA in WebLogic Kubernetes Operator Environment.",
	"content": "This documents provides steps to deploy SOA composite applications from Oracle JDeveloper (that runs outside the Kubernetes network) to the SOA instance in WebLogic Kubernetes Operator Environment.\nNote: Dev and Test environment only. For production you should deploy using Application Control and WLST methods. Deploy to SOA from JDeveloper To deploy SOA composites/applications from Oracle JDeveloper, Administration Server should have been configured to expose a T3 channel using the exposeAdminT3Channel setting when creating the domain, then the matching T3 service can be used to connect.\nBy default when exposeAdminT3Channel is set WebLogic Kubernetes Operator Environment will expose NodePort for the T3 channel of the NetworkAccessPoint at 30012 (Use t3ChannelPort to configure port to different value).\nPrerequisities Note: Replace entries inside specific to your environment\nNOTE : The Managed Server t3 port is not exposed by default and opening this will have a security risk as the authentication method here is based on a userid/password. It is not recommended to do this on production instances.\n  Get the Kubernetes Cluster Master Address and verify the T3 port which will be used for creating application server connections. You can use below kubectl command to get the T3 port:\n$ kubectl get service \u0026lt;domainUID\u0026gt;-\u0026lt;AdministrationServerName\u0026gt;-external -n \u0026lt;namespace\u0026gt;-o jsonpath='{.spec.ports[0].nodePort}'    JDeveloper need to access Managed Server during deployment. In WebLogic operator Environment each Managed Servers are pods and cannot be accessed directly by JDeveloper. Hence we need to configure the Managed Server\u0026rsquo;s reachability:\na. Decide on external IP address to be used to configure access of Managed Server ( soa cluster). Master or worker node IP address can be used to configure Managed Server accessibility. In case you decide to use some other external IP address, that need to be accessible from Kubernetes Cluster. Here we will be using Kubernetes Cluster Master IP.\nb. Get the pod names of Administration and Managed Servers (i.e. \u0026ldquo;\u0026lt;domainUID\u0026gt;-\u0026lt;server name\u0026gt;\u0026rdquo;) which will be used to map in /etc/hosts.\nc. Update /etc/hosts (or in Windows: C:\\Windows\\System32\\Drivers\\etc\\hosts) on the host from where JDeveloper is running with below entires where\n\u0026lt;Master IP\u0026gt; \u0026lt;Administration Server pod name\u0026gt; \u0026lt;Master IP\u0026gt; \u0026lt;Managed Server1 pod name\u0026gt; \u0026lt;Master IP\u0026gt; \u0026lt;Managed Server2 pod name\u0026gt;  d. Get the Kubernetes service name of the SOA Cluster so that we can make them access externally with Master IP ( or External IP).\n$ kubectl get service \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt; -n \u0026lt;namespace\u0026gt;  e. Create a Kubernetes service to expose SOA cluster service (“-cluster-”) to available externally with same port of Managed Server:\n$ kubectl expose service \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;soa-cluster\u0026gt; --name \u0026lt;domainUID\u0026gt;-\u0026lt;soa-cluster\u0026gt;-ext --external-ip=\u0026lt;Master IP\u0026gt; -n \u0026lt;namespace\u0026gt;    Create an Application Server Connection in JDeveloper   Create a new application server connection in JDeveloper\n  In the configuration page provide the WebLogic Hostname as Kubernetes Master Address\n  Update the Port as T3 port ( default is 30012) obtained in the prerequisites step 1\n  Enter the WebLogic Domain i.e (domainUID)\n  Test the Connection and it should be successful without any error\n  Deployment of SOA Composites to SOA using JDeveloper   In JDeveloper, right click the SOA project you want to deploy and select the Deploy menu. This invokes the deployment wizard\n  In the deployment wizard, select the application server connection that was created earlier.\nIf the prerequisites has been configured correctly, the next step looks up the SOA servers and shows the Managed Servers for deploying the composite.\n  Using the application server connection, Managed Servers (SOA cluster) are discovered and get listed on the select servers page. Select the SOA cluster and click Next.\n  On Summary page, click Finish to start deploying the composites to SOA cluster.\n  Once deployment is successful, verify with soa-infra URL to confirm the composites are deployed on both servers:\n  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/patch-soa-domains/",
	"title": "Patch SOA domains",
	"tags": [],
	"description": "Patches information for Oracle SOA Suite domains.",
	"content": "We recommend to deploy the Oracle SOA Suite domains with the latest Oracle SOA Suite bundle patch applied. The latest Oracle SOA Suite bundle patch is 30638101 SOA Bundle Patch 12.2.1.4.191208.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/manage-soa-domains/enablingt3/",
	"title": "Expose T3 protocol for Managed Servers",
	"tags": [],
	"description": "Learn how to create a T3 channel and the corresponding Kubernetes Service to expose the T3 protocol for Managed Servers in SOA Domain.",
	"content": "T3 ports for Managed Servers in Oracle SOA deployed in WebLogic Kubernetes operator Environment are not available by default. This document provides steps to create T3 channel and the corresponding Kubernetes Service to expose the T3 protocol for Managed Servers in SOA Domain.\nWith the following steps you will be creating T3 port at 30014 on all Managed Servers for soa_cluster with below details:\n Name: T3Channel_MS Listen Port: 30014 External Listen Address: \u0026lt;Master IP Address\u0026gt; External Listen Port: 30014  Note: In case you are using different NodePort to expose T3 for Managed Server externally, then use same value for \u0026quot;External Listen Port\u0026quot;\nStep 1 : Create T3 Channels for Managed Servers WebLogic Server supports several ways to configure T3 channel. Below steps describe the methods to create T3 channel using WebLogic Server Administration Console or using WLST Scripts.\nMethod 1 : Using WebLogic Server Administration Console   Login to WebLogic Server Administration Console and obtain the configuration lock by clicking on Lock \u0026amp; Edit\n  In the left pane of the Console, expand Environment and select Servers.\n  On the Servers page, click on the soa_server1 and go to Protocols page\n  Select Channel and then Click \u0026ldquo;New\u0026rdquo;\n  Enter the Network Channel Name as say \u0026ldquo;T3Channel_MS\u0026rdquo;, Select Protocol as \u0026ldquo;t3\u0026rdquo; and Click \u0026ldquo;Next\u0026rdquo;\n  Enter Listen Port as \u0026ldquo;30014\u0026rdquo;, External Listen Address as \u0026quot;\u0026lt;Master IP\u0026gt;\u0026rdquo; and External Listen Port as \u0026ldquo;30014\u0026rdquo;. Leave empty for \u0026ldquo;Listen Address\u0026rdquo;. Click \u0026ldquo;Finish\u0026rdquo; to create the Network Channel for soa_server1.\n  Perform step 3 to 6 for all Managed Servers in soa_cluster. When creating Network Channel for other Managed Servers, make sure to use same values as for all parameters including \u0026ldquo;Network Channel Name\u0026rdquo;.\n  To activate these changes, in the Change Center of the Administration Console, click Activate Changes.\n  These changes does not require any server restarts. Once the T3 channels are created with port say 30014, proceed with creating the Kubernetes Service to access this port externally.\n  Method 2 : Using WLST Script The following steps creates a custom T3 channel for all Managed Servers with name T3Channel_MS that has a listen port listen_port and a paired public port public_port.\n Create t3config_ms.py with below content:  host = sys.argv[1] port = sys.argv[2] user_name = sys.argv[3] password = sys.argv[4] listen_port = sys.argv[5] public_port = sys.argv[6] public_address = sys.argv[7] managedNameBase = sys.argv[8] ms_count = sys.argv[9] print(\u0026#39;custom host : [%s]\u0026#39; % host); print(\u0026#39;custom port : [%s]\u0026#39; % port); print(\u0026#39;custom user_name : [%s]\u0026#39; % user_name); print(\u0026#39;custom password : ********\u0026#39;); print(\u0026#39;public address : [%s]\u0026#39; % public_address); print(\u0026#39;channel listen port : [%s]\u0026#39; % listen_port); print(\u0026#39;channel public listen port : [%s]\u0026#39; % public_port); connect(user_name, password, \u0026#39;t3://\u0026#39; + host + \u0026#39;:\u0026#39; + port) edit() startEdit() for index in range(0, int(ms_count)): cd(\u0026#39;/\u0026#39;) msIndex = index+1 cd(\u0026#39;/\u0026#39;) name = \u0026#39;%s%s\u0026#39; % (managedNameBase, msIndex) cd(\u0026#39;Servers/%s/\u0026#39; % name ) create(\u0026#39;T3Channel_MS\u0026#39;,\u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/T3Channel_MS\u0026#39;) set(\u0026#39;Protocol\u0026#39;,\u0026#39;t3\u0026#39;) set(\u0026#39;ListenPort\u0026#39;,int(listen_port)) set(\u0026#39;PublicPort\u0026#39;,int(public_port)) set(\u0026#39;PublicAddress\u0026#39;, public_address) print(\u0026#39;Channel T3Channel_MS added ...for \u0026#39; + name) activate() disconnect()  Copy t3config_ms.py into Domain Home (e.g., /u01/oracle/user_projects/domains/soainfra) of Administration Server pod (e.g., soainfra-adminserver in soans namespace)\n $ kubectl cp t3config_ms.py soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra    Execute wlst.sh t3config_ms.py by exec into Administration Server pod with below parameters\n host: \u0026lt;Master IP Address\u0026gt; port: 30012 # Administration Server T3 port user_name: weblogic password: Welcome1 # weblogic password listen_port: 30014 # New port for T3 Managed Servers public_port: 30014 # Kubernetes NodePort which will be used to expose T3 port externally public_address: \u0026lt;Master IP Address\u0026gt; managedNameBase: soa_server # Give Managed Server base name. For osb_cluster this will be osb_server ms_count: 5 # Number of configured Managed Servers  Command: $ kubectl exec -it \u0026lt;Administration Server pod\u0026gt; -n \u0026lt;namespace\u0026gt; -- /u01/oracle/oracle_common/common/bin/wlst.sh \u0026lt;domain_home\u0026gt;/t3config_ms.py \u0026lt;master_ip\u0026gt; \u0026lt;t3 port on Administration Server\u0026gt; weblogic \u0026lt;password for weblogic\u0026gt; \u0026lt;t3 port on Managed Server\u0026gt; \u0026lt;t3 nodeport\u0026gt; \u0026lt;master_ip\u0026gt; \u0026lt;managedNameBase\u0026gt; \u0026lt;ms_count\u0026gt;  Sample Command: $ kubectl exec -it soainfra-adminserver -n soans -- /u01/oracle/oracle_common/common/bin/wlst.sh /u01/oracle/user_projects/domains/soainfra/t3config_ms.py xxx.xxx.xxx.xxx 30012 weblogic Welcome1 30014 30014 xxx.xxx.xxx.xxx soa_server 5    Step 2 : Create Kubernetes Service to expose T3 port 30014 as NodePort Service   Create t3_ms_svc.yaml with below contents to expose T3 at Managed Server port 30014 for domainName and domainUID as \u0026ldquo;soainfra\u0026rdquo; and cluster as \u0026ldquo;soa_cluster\u0026rdquo; :\napiVersion: v1 kind: Service metadata: name: soainfra-soa-cluster-t3-external namespace: soans labels: weblogic.clusterName: soa_cluster weblogic.domainName: soainfra weblogic.domainUID: soainfra spec: type: NodePort selector: weblogic.clusterName: soa_cluster weblogic.domainName: soainfra weblogic.domainUID: soainfra ports: - name: t3port protocol: TCP port: 30014 targetPort: 30014 nodePort: 30014   Create the NodePort Service for port 30014 with command:\n $ kubectl create -f t3_ms_svc.yaml    Now you can access t3 for Managed Server with below URL\n t3://\u0026lt;master_ip\u0026gt;:30014    "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/known-issues/",
	"title": "Known Issues in WCSites",
	"tags": [],
	"description": "Known Issues for Oracle WebCenter Sites domains",
	"content": "Known Issues for Oracle WebCenter Sites domains\nKnown issues    Issue Description     Publishing via LoadBalancer Endpoint Currenly publishihng is only supported via NodePort as described in section For Publishing Setting in WebCenter Sites on page.    "
},
{
	"uri": "/fmw-kubernetes/soa-domains/known-issues/",
	"title": "Known Issues",
	"tags": [],
	"description": "Known Issues for Oracle SOA Suite domains",
	"content": "This document describes the known issues for Oracle SOA Suite domains deployed on Kubernetes.\n"
},
{
	"uri": "/fmw-kubernetes/soa-domains/manage-soa-domains/persisting-soa-adapters-customizations/",
	"title": "Persisting SOA Adapters Customizations",
	"tags": [],
	"description": "The lifetime for any customization done in a file on a server pod is upto the lifetime of that pod, the changes are not persisted once the pod goes down or restarted. This document describes the steps for persisting the customizations done for SOA Adapters.",
	"content": "The lifetime for any customization done in a file on a server pod is upto the lifetime of that pod, the changes are not persisted once the pod goes down or restarted.\nFor example: Below configuration updates DbAdapter.rar to create a new connection instance and creates DatasSource CoffeeShop on Administration Console for the same with jdbc/CoffeeShopDS.\nfile location: /u01/oracle/soa/soa/connectors/DbAdapter.rar\n\u0026lt;connection-instance\u0026gt; \u0026lt;jndi-name\u0026gt;eis/DB/CoffeeShop\u0026lt;/jndi-name\u0026gt; \u0026lt;connection-properties\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;XADataSourceName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc/CoffeeShopDS\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;DataSourceName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;PlatformClassName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.eclipse.persistence.platform.database.Oracle10Platform\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/connection-properties\u0026gt; \u0026lt;/connection-instance\u0026gt; If you need to persist the customizations for any of the adpater files under SOA oracle home in the server pod, you need to follow one of the below methods.\nMethod 1: Customize the Adapter file using the Administration console:  Login to WebLogic Administration console : Deployments -\u0026gt; ABC.rar -\u0026gt; Configuration -\u0026gt; Outbound Connection Pools Create a new connection that is required : New -\u0026gt; provide connection name -\u0026gt; finish Go back to this new connection : update the required properties under it and save Go back to deployments : select the ABC.rar -\u0026gt; Update This step asks for Plan.xml location. This location by default will be in ${MW_HOME}/soa/soa which is not under Persistent Volume.\nHence when you specify above location, provide the domains PV location such as {DOMAIN_HOME}/soainfra/servers etc.\nNow the Plan.xml will be persisted under this location for each Managed Servers.  Method 2: Customize the Adapter file on the Worker Node:  Copy the ABC.rar from the server pod to a PV path: command: $ kubectl cp \u0026lt;namespace\u0026gt;/\u0026lt;SOA Managed Server pod name\u0026gt;:\u0026lt;full path of .rar file\u0026gt; \u0026lt;destination path inside PV\u0026gt; Sample command: $ kubectl cp soans/soainfra-soa-server1:/u01/oracle/soa/soa/connectors/ABC.rar ${DockerVolume}/domains/soainfra/servers/ABC.rar or You can do a normal file copy between these locations after entering (kubectl exec) in to the Managed Server pod.\n Unrar the ABC.rar. Update the new connection details in weblogic-ra.xml file under META_INF. In WebLogic Administration console, Under Deployments -\u0026gt; select ABC.rar and click update. Here, select the ABC.rar path as the new location which is: ${DOMAIN_HOME}/user_projects/domains/soainfra/servers/ABC.rar and update Verify that the plan.xml or updated .rar should be persisted in PV.  "
},
{
	"uri": "/fmw-kubernetes/soa-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/fmw-kubernetes/soa-domains/",
	"title": "Oracle SOA Suite",
	"tags": [],
	"description": "The WebLogic Kubernetes operator supports deployment of SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS). Follow the instructions in this guide to set up these Oracle SOA Suite domains on Kubernetes.",
	"content": "The WebLogic Kubernetes operator supports deployment of SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS).\nOracle SOA Suite is currently supported for non-production use only in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle SOA Suite in Kubernetes before it is supported for production use.\n In this release, SOA Suite domains are supported using the “domain on a persistent volume” model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing SOA domains in a Kubernetes environment. You can:\n Create SOA instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the SOA Services and Composites for external access. Scale SOA domains by starting and stopping Managed Servers on demand, or by integrating with a REST API to initiate scaling based on WLDF, Prometheus, Grafana, or other rules. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the SOA instance using Prometheus and Grafana.  Limitations See here for limitations in this release.\nGetting started For detailed information about deploying Oracle SOA Suite domains, see the User guide.\nCurrent release The current release of the operator is 2.4.0.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/",
	"title": "Oracle WebCenter Sites",
	"tags": [],
	"description": "The WebLogic Kubernetes operator supports deployment of Oracle WebCenter Sites. Follow the instructions in this guide to set up Oracle WebCenter Sites domains on Kubernetes.",
	"content": "The WebLogic Kubernetes operator supports deployment of Oracle WebCenter Sites.\nOracle WebCenter Sites is currently supported only for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle WebCenter Sites in Kubernetes before it is supported for production use.\n In this release, Oracle WebCenter Sites domains are supported using the domain on a persistent volume model only, where the domain home is located in a persistent volume (PV).\nThe operator has several key features to assist you with deploying and managing Oracle WebCenter Sites domains in a Kubernetes environment. You can:\n Create Oracle WebCenter Sites instances in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Start servers based on declarative startup parameters and desired states. Expose the WebCenter Sites Services and Composites for external access. Scale WebCenter Sites domains by starting and stopping Managed Servers on demand, or by integrating with a REST API to initiate scaling based on WLDF, Prometheus, Grafana, or other rules. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. Monitor the WebCenter Sites instance using Prometheus and Grafana.  Limitations Refer here for limitations in this release.\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/loadbalancer-traefik-setup-for-wcsites-domain-setup-on-k8s/",
	"title": "a. Using Traefik Loadbalancer",
	"tags": [],
	"description": "Steps to set up Traefik as a loadbalancer for the WebCenter Sites domain.",
	"content": "Setting Up Loadbalancer Traefik for the WebCenter Sites Domain on K8S The Oracle WebLogic Server Kubernetes Operator supports three load balancers: Traefik, Voyager, and Apache. Follow these steps to set up Traefik as a loadbalancer for the WebCenter Sites domain:\n Install the Traefik Load Balancer Configure Traefik to Manage Ingresses Create an Ingress for the Domain Verify that You can Access the Domain URL  Install the Traefik Load Balancer   Use helm to install the Traefik load balancer. For detailed information, see this document. Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n Install Traefik\n $ cd weblogic-kubernetes-operator $ helm install stable/traefik --name traefik-operator \\  --namespace traefik --values kubernetes/samples/charts/traefik/values.yaml \\  --set \u0026#34;dashboard.domain=$(hostname -f),kubernetes.namespaces={traefik}\u0026#34;  Output\n $ cd weblogic-kubernetes-operator $ helm install stable/traefik --name traefik-operator --namespace traefik --values kubernetes/samples/charts/traefik/values.yaml --set \u0026#34;dashboard.domain=$(hostname -f),kubernetes.namespaces={traefik}\u0026#34; --wait NAME: traefik-operator LAST DEPLOYED: Sat Mar 14 13:53:16 2020 NAMESPACE: traefik STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik-operator-dashboard ClusterIP 10.108.89.215 \u0026lt;none\u0026gt; 80/TCP 0s traefik-operator NodePort 10.99.75.162 \u0026lt;none\u0026gt; 443:30443/TCP,80:30305/TCP 0s ==\u0026gt; v1/Secret NAME TYPE DATA AGE traefik-operator-default-cert Opaque 2 0s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE traefik-operator 1 0s ==\u0026gt; v1/RoleBinding NAME AGE traefik-operator 0s ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE traefik-operator-dashboard abc.def.com 80 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE traefik-operator-844859fdd6-prh55 0/1 ContainerCreating 0 0s ==\u0026gt; v1/ConfigMap NAME DATA AGE traefik-operator 1 0s ==\u0026gt; v1/Role NAME AGE traefik-operator 0s ==\u0026gt; v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE traefik-operator 1 1 1 0 0s NOTES: 1. Traefik is listening on the following ports on the host machine: http - 30305 https - 30443 2. Configure DNS records corresponding to Kubernetes ingress resources to point to the NODE_IP/NODE_HOST   Access the Traefik dashboard through the URL http://$(hostname -f):30305, with the HTTP host traefik.example.com. NOTE: Make sure you specify full qualified node name for $(hostname -f).\n$ curl -H \u0026#39;host: $(hostname -f)\u0026#39; http://$(hostname -f):30305/ \u0026lt;a href=\u0026#34;/dashboard/\u0026#34;\u0026gt;Found\u0026lt;/a\u0026gt;. $   Configure Traefik to Manage Ingresses Configure Traefik to manage Ingresses created in this namespace: Note: Here traefik is the Traefik namespace, wcsites-ns is the namespace of the domain.\n helm upgrade for traefik\n $ helm upgrade --reuse-values --set \u0026#34;kubernetes.namespaces={traefik,wcsites-ns}\u0026#34; --wait traefik-operator stable/traefik Release \u0026#34;traefik-operator\u0026#34; has been upgraded. Happy Helming! LAST DEPLOYED: Sat Mar 14 13:58:03 2020 NAMESPACE: traefik STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ConfigMap NAME DATA AGE traefik-operator 1 5m2s ==\u0026gt; v1/ClusterRole NAME AGE traefik-operator 14s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik-operator-dashboard ClusterIP 10.108.89.215 \u0026lt;none\u0026gt; 80/TCP 5m2s traefik-operator NodePort 10.99.75.162 \u0026lt;none\u0026gt; 443:30443/TCP,80:30305/TCP 5m2s ==\u0026gt; v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE traefik-operator 1 1 1 1 5m2s ==\u0026gt; v1/Secret NAME TYPE DATA AGE traefik-operator-default-cert Opaque 2 5m2s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE traefik-operator 1 5m2s ==\u0026gt; v1/ClusterRoleBinding NAME AGE traefik-operator 14s ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE traefik-operator-dashboard abc.def.com 80 5m2s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE traefik-operator-844699994b-ttfb6 1/1 Running 0 14s traefik-operator-844859fdd6-prh55 0/1 Terminating 0 5m2s NOTES: 1. Traefik is listening on the following ports on the host machine: http - 30305 https - 30443 2. Configure DNS records corresponding to Kubernetes ingress resources to point to the NODE_IP/NODE_HOST Create an Ingress for the Domain   Create an Ingress for the domain (ingress-per-domain-wcsites), in the domain namespace by using the sample Helm chart. Here we are using the path-based routing for ingress. For detailed instructions about ingress, see this page).\nFor now, you can update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml with appropriate values. Sample values are shown below:\n$ cat kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml # Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: TRAEFIK, VOYAGER type: TRAEFIK #type: VOYAGER # WLS domain as backend to the load balancer wlsDomain: domainUID: wcsitesinfra adminServerName: adminserver adminServerPort: 7001 wcsitesClusterName: wcsites_cluster wcsitesManagedServerPort: 8001 # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30317   Update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/templates/traefik-ingress.yaml with the url routes to be load balanced.\nBelow are the defined ingress rules:\nNOTE: This is not an exhaustive list of rules. You can enhance it based on the application urls that need to be accessed externally. These rules hold good for domain type WCSITES.\n$ vi kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/templates/traefik-ingress.yaml # Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. {{- if eq .Values.type \u0026#34;TRAEFIK\u0026#34; }} --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: {{ .Values.wlsDomain.domainUID }}-traefik namespace: {{ .Release.Namespace }} labels: weblogic.resourceVersion: domain-v2 spec: annotations: kubernetes.io/ingress.class: traefik rules: - host: \u0026#39;{{ .Values.traefik.hostname }}\u0026#39; http: paths: - path: /console backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /em backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /wls-exporter backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /weblogic backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sbconsole backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sites backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} - path: /cas backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} - path: /wls-exporter backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} # - path: /wls-cat # backend: # serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; # servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} # - path: # backend: # serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; # servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} {{- end }}   Install \u0026ldquo;ingress-per-domain\u0026rdquo; using helm.\nbash-4.2$ cd weblogic-kubernetes-operator bash-4.2$ helm install kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain \\  --name wcsitesinfra-ingress --namespace wcsites-ns \\  --values kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml \\  --set \u0026#34;traefik.hostname=$(hostname -f)\u0026#34; NAME: wcsitesinfra-ingress LAST DEPLOYED: Sat Mar 14 14:03:22 2020 NAMESPACE: wcsites-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE wcsitesinfra-traefik abc.def.com 80 0s   To confirm that the load balancer noticed the new Ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which should return a HTTP 200 status code, as shown in the example below:\n  -bash-4.2$ curl -v http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready * Trying 149.87.129.203... \u0026gt; GET http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; Proxy-Connection: Keep-Alive \u0026gt; host: $(hostname -f) \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Sat, 14 Mar 2020 08:35:03 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Proxy-Connection: Keep-Alive \u0026lt; * Connection #0 to host localhost left intact Verify that You can Access the Domain URL After setting up the Traefik loadbalancer, verify that the domain applications are accessible through the loadbalancer port 30305. Through load balancer (Traefik port 30305), the following URLs are available for setting up domains of WebCenter Sites domain types:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/version.jsp "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/loadbalancer-voyager-setup-for-wcsites-domain-setup-on-k8s/",
	"title": "b. Using Voyager Loadbalancer",
	"tags": [],
	"description": "Steps to set up Voyager as a loadbalancer for the WebCenter Sites domain.",
	"content": "Setting Up Loadbalancer Voyager for the WebCenter Sites Domain on K8S The Oracle WebLogic Server Kubernetes Operator supports three load balancers: Traefik, Voyager, and Apache. Follow these steps to set up Voyager as a loadbalancer for the WebCenter Sites domain:\n Install the Voyager Load Balancer Configure Voyager to Manage Ingresses Verify that You can Access the Domain URL  Install the Voyager Load Balancer See the official installation document and follow step 1 and 2 here.\nConfigure Voyager to Manage Ingresses   Create an Ingress for the domain (ingress-per-domain) in the domain namespace, by using the sample Helm chart.\nHere we are using the path based routing for ingress. For detailed instructions about ingress, refer this page.\nFor this update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml with appropriate values, sample values are shown below:\n-bash-4.2$ cat kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml type: VOYAGER # Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: TRAEFIK, VOYAGER #type: TRAEFIK type: VOYAGER # WLS domain as backend to the load balancer wlsDomain: domainUID: wcsitesinfra adminServerName: adminserver adminServerPort: 7001 wcsitesClusterName: wcsites_cluster wcsitesManagedServerPort: 8001 # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30317   Update the kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/templates/voyager-ingress.yaml with the url routes to be load balanced.\nBelow are the ingress rules defined:\nNOTE: These are not the exhausted list of rules. These can be enhanced based on the application urls that needs to be accessed externally.\nBelow rules hold good for domain type WCSITES.\n# Copyright 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. {{- if eq .Values.type \u0026#34;VOYAGER\u0026#34; }} --- apiVersion: voyager.appscode.com/v1beta1 kind: Ingress metadata: name: {{ .Values.wlsDomain.domainUID }}-voyager namespace: {{ .Release.Namespace }} annotations: ingress.appscode.com/type: \u0026#39;NodePort\u0026#39; ingress.appscode.com/stats: \u0026#39;true\u0026#39; ingress.appscode.com/affinity: \u0026#39;cookie\u0026#39; ingress.appscode.com/default-timeout: \u0026#39;{\u0026#34;connect\u0026#34;: \u0026#34;1800s\u0026#34;, \u0026#34;server\u0026#34;: \u0026#34;1800s\u0026#34;}\u0026#39; spec: rules: - host: \u0026#39;*\u0026#39; http: nodePort: {{ .Values.voyager.webPort }} paths: - path: /console backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /em backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} # - path: /wls-exporter # backend: # serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} # servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /weblogic backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sbconsole backend: serviceName: {{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /sites backend: serviceName: {{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} - path: /cas backend: serviceName: {{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} # - path: /wls-exporter # backend: # serviceName: {{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.wcsitesClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} # servicePort: {{ .Values.wlsDomain.wcsitesManagedServerPort }} --- apiVersion: v1 kind: Service metadata: name: {{ .Values.wlsDomain.domainUID }}-voyager-stats namespace: {{ .Release.Namespace }} spec: type: NodePort ports: - name: client protocol: TCP port: 56789 targetPort: 56789 nodePort: {{ .Values.voyager.statsPort }} selector: origin: voyager origin-name: {{ .Values.wlsDomain.domainUID }}-voyager {{- end }}   Install ingress-per-domain using helm.\nbash-4.2$ cd weblogic-kubernetes-operator -bash-4.2$ helm install kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain \\  --name wcsitesinfra-voyager-ingress --namespace wcsites-ns \\  --values kubernetes/samples/scripts/create-wcsites-domain/ingress-per-domain/values.yaml NAME: wcsitesinfra-voyager-ingress LAST DEPLOYED: Fri Feb 14 13:20:17 2020 NAMESPACE: wcsites-ns STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wcsitesinfra-voyager-stats NodePort 10.101.94.249 \u0026lt;none\u0026gt; 56789:30317/TCP 0s ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE wcsitesinfra-ingress yourcompany-loadbalancer.com 80 0s NAME AGE wcsitesinfra-voyager 0s   To confirm that the load balancer noticed the new Ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which should return a HTTP 200 status code, as shown in the example below:\n  -bash-4.2$ curl -v http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready * About to connect() to localhost port 30305 (#0) * Trying 127.0.0.1... * Connected to localhost (127.0.0.1) port 30305 (#0) \u0026gt; GET /weblogic/ready HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; host: *****.com \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 12 Mar 2020 10:16:43 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; * Connection #0 to host localhost left intact Verify that You can Access the Domain URL After setting up the Traefik loadbalancer, verify that the domain applications are accessible through the loadbalancer port 30305. Through load balancer (Traefik port 30305), the following URLs are available for setting up domains of WebCenter Sites domain types:\nhttp://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/weblogic/ready http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/console http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/em http://${LOADBALANCER-HOSTNAME}:${LOADBALANCER-PORT}/sites/version.jsp "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/elasticsearch-integration-with-wls-operator-and-wls-server-logs/",
	"title": "c. ELK Integration for Logs",
	"tags": [],
	"description": "",
	"content": "1. Integrate Elasticsearch to WebLogic Kubernetes Operator For reference information, see Elasticsearch integration for the WebLogic Kubernetes Operator.\nTo enable elasticsearch integration, you must edit file kubernetes/charts/weblogic-operator/values.yaml before deploying the WebLogic Kubernetes Operator.\n# elkIntegrationEnabled specifies whether or not ELK integration is enabled. elkIntegrationEnabled: true # logStashImage specifies the docker image containing logstash. # This parameter is ignored if 'elkIntegrationEnabled' is false. logStashImage: \u0026quot;logstash:6.6.0\u0026quot; # elasticSearchHost specifies the hostname of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchHost: \u0026quot;elasticsearch.default.svc.cluster.local\u0026quot; # elasticSearchPort specifies the port number of where Elasticsearch is running. # This parameter is ignored if 'elkIntegrationEnabled' is false. elasticSearchPort: 9200 After you\u0026rsquo;ve deployed WebLogic Kubernetes Operator and made the above changes, the weblogic-operator pod will have additional Logstash container. The Logstash container will push the weblogic-operator logs to the configured Elasticsearch server.\n2. Publish WebLogic Server and WebCenter Sites Logs using Logstash Pod You can publish the WebLogic Server logs to Elasticsearch Server using Logstash pod. This Logstash pod must have access to the shared domain home. For the WebCenter Sites wcsitesinfra, you can use the persistent volume of the domain home in the Logstash pod. The steps to create the Logstash pod are as follows:\nSample Logstash configuration file is located at kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.conf\n$ vi kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.conf input { file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/AdminServer.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/wcsites_server*.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/AdminServer.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/logs/wcsitesinfra/wcsites_server*.out\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wcsitesinfra/servers/**/logs/sites.log\u0026quot; start_position =\u0026gt; beginning } file { path =\u0026gt; \u0026quot;/u01/oracle/user_projects/domains/wcsitesinfra/servers/**/logs/cas.log\u0026quot; start_position =\u0026gt; beginning } } filter { grok { match =\u0026gt; [ \u0026quot;message\u0026quot;, \u0026quot;\u0026lt;%{DATA:log_timestamp}\u0026gt; \u0026lt;%{WORD:log_level}\u0026gt; \u0026lt;%{WORD:thread}\u0026gt; \u0026lt;%{HOSTNAME:hostname}\u0026gt; \u0026lt;%{HOSTNAME:servername}\u0026gt; \u0026lt;%{DATA:timer}\u0026gt; \u0026lt;\u0026lt;%{DATA:kernel}\u0026gt;\u0026gt; \u0026lt;\u0026gt; \u0026lt;%{DATA:uuid}\u0026gt; \u0026lt;%{NUMBER:timestamp}\u0026gt; \u0026lt;%{DATA:misc}\u0026gt; \u0026lt;%{DATA:log_number}\u0026gt; \u0026lt;%{DATA:log_message}\u0026gt;\u0026quot; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;elasticsearch.default.svc.cluster.local:9200\u0026quot;] } } Here ** means that all sites.log and cas.log from any servers under wcsitesinfra will be pushed to Logstash.\n$ kubectl cp kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.conf wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/logs/logstash.conf Get the persistent volume details of the domain home of the WebLogic Server(s). The following command will list the persistent volume details in the namespace - \u0026ldquo;wcsites-ns\u0026rdquo;:\n$ kubectl get pv -n wcsites-ns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE wcsitesinfra-domain-pv 10Gi RWX Retain Bound wcsites-ns/wcsitesinfra-domain-pvc wcsitesinfra-domain-storage-class 5d21h Sample Logstash deployment is located at kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.yaml for Logstash pod. The mounted persistent volume of the domain home will provide access to the WebLogic Server logs to Logstash pod.\napiVersion: apps/v1beta1 kind: Deployment metadata: name: logstash-wls namespace: wcsites-ns spec: template: # create pods using pod definition in this template metadata: labels: k8s-app: logstash-wls spec: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: wcsitesinfra-domain-pvc - name: shared-logs emptyDir: {} containers: - name: logstash image: logstash:6.6.0 command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;/usr/share/logstash/bin/logstash\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/u01/oracle/user_projects/domains/logs/logstash.conf\u0026quot;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /u01/oracle/user_projects/domains name: weblogic-domain-storage-volume - name: shared-logs mountPath: /shared-logs ports: - containerPort: 5044 name: logstash After you have created the Logstash deployment yaml and Logstash configuration file, deploy Logstash using following command:\n$ kubectl create -f kubernetes/samples/scripts/create-wcsites-domain/utils/logstash/logstash.yaml 3. Test the Deployment of Elasticsearch and Kibana The WebLogic Operator also provides a sample deployment of Elasticsearch and Kibana for testing purpose. You can deploy Elasticsearch and Kibana on the Kubernetes cluster as shown below:\n$ kubectl create -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml Get the Kibana dashboard port information as shown below: Wait for pods to start:\n-bash-4.2$ kubectl get pods -w NAME READY STATUS RESTARTS AGE elasticsearch-8bdb7cf54-mjs6s 1/1 Running 0 4m3s kibana-dbf8964b6-n8rcj 1/1 Running 0 4m3s -bash-4.2$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.100.11.154 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 4m32s kibana NodePort 10.97.205.0 \u0026lt;none\u0026gt; 5601:31884/TCP 4m32s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 71d You can access the Kibana dashboard at http://mycompany.com:kibana-nodeport/. In our example, the node port would be 31884.\nCreate an Index Pattern in Kibana Create an index pattern logstash* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/weblogic-logging-exporter-setup/",
	"title": "d. Publish logs to Elasticsearch",
	"tags": [],
	"description": "Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server. WebLogic Server logs can be pushed to Elasticsearch in Kubernetes directly by using the Elasticsearch REST API. For more details, see to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please see this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-1.0.0.jar from the Releases page. snakeyaml-1.25.jar from Maven Central.  These identifiers are used in the sample commands in this document.\n wcsites-ns: WebCenter Sites domain namespace wcsitesinfra: domainUID wcsitesinfra-adminserver: Administration Server pod name   Copy the JAR Files to the WebLogic Domain Home Copy the weblogic-logging-exporter-1.0.0.jar and snakeyaml-1.25.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp snakeyaml-1.25.jar wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ $ kubectl cp weblogic-logging-exporter-1.0.0.jar wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ Add a Startup Class to the Domain Configuration   In the WebLogic Server Administration Console, in the left navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  In your /u01/oracle/user_projects/domains/wcsitesinfra/config/config.xml file, this update should look similar to the following example:\n$ kubectl exec -it wcsitesinfra-adminserver -n wcsites-ns cat /u01/oracle/user_projects/domains/wcsitesinfra/config/config.xml \u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,wcsites_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt;   Update the WebLogic Server CLASSPATH   Copy the setDomainEnv.sh file from the pod to a local folder:\n$ kubectl cp wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/bin/setDomainEnv.sh $PWD/setDomainEnv.sh tar: Removing leading `/' from member names Ignore exception: tar: Removing leading '/' from member names\n  Update the server class path in setDomainEnv.sh:\nCLASSPATH=/u01/oracle/user_projects/domains/wcsitesinfra/weblogic-logging-exporter-1.0.0.jar:/u01/oracle/user_projects/domains/wcsitesinfra/snakeyaml-1.25.jar:${CLASSPATH} export CLASSPATH   Copy back the modified setDomainEnv.sh file to the pod:\n$ kubectl cp setDomainEnv.sh wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/bin/setDomainEnv.sh ``\n  Create a Configuration File for the WebLogic Logging Exporter   Specify the Elasticsearch server host and port number in file kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml:\nExample:\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9200 domainUID: wcsitesinfra weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: TRACE weblogicLoggingExporterBulkSize: 1   Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Administration Server pod:\n$ kubectl cp kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-logging-exporter/WebLogicLoggingExporter.yaml wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/config/   Restart All the Servers in the Domain To restart the servers, stop and then start them using the following commands:\nTo stop the servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NEVER\u0026quot; }]' To start the servers:\n$ kubectl patch domain wcsitesinfra -n wcsites-ns --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;IF_NEEDED\u0026quot; }]' After all the servers are restarted, see their server logs to check that the weblogic-logging-exporter class is called, as shown below:\n======================= WebLogic Logging Exporter Startup class called Reading configuration from file name: /u01/oracle/user_projects/domains/wcsitesinfra/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='domain.host.com', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='2', enabled=true, weblogicLoggingExporterFilters=FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='wcsitesinfra'} Create an Index Pattern in Kibana Create an index pattern wls* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard:\n"
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/manage-wcsites-domains/weblogic-monitoring-exporter-setup/",
	"title": "e. Monitor a WebCenter Sites domain",
	"tags": [],
	"description": "Use the WebLogic Monitoring Exporter to monitor a WebCenter Sites instance using Prometheus and Grafana.",
	"content": "You can monitor a WebCenter Sites domain using Prometheus and Grafana by exporting the metrics from the domain instance using the WebLogic Monitoring Exporter. This sample shows you how to set up the WebLogic Monitoring Exporter to push the data to Prometheus.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nClone the kube-prometheus project $ git clone https://github.com/coreos/kube-prometheus.git Label the nodes Kube-Prometheus requires all the exporter nodes to be labelled with kubernetes.io/os=linux. If a node is not labelled, then you must label it using the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux Create Prometheus and Grafana resources Change to the kube-prometheus directory and execute the following commands to create the namespace and CRDs:\nNOTE: Wait for a minute for each command to process.\n$ cd kube-prometheus $ kubectl create -f manifests/setup $ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \u0026#34;\u0026#34;; done $ kubectl create -f manifests/ Provide external access To provide external access for Grafana, Prometheus, and Alertmanager, execute the commands below:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; NOTE:\n 32100 is the external port for Grafana 32101 is the external port for Prometheus 32102 is the external port for Alertmanager   Set Up the WebLogic Monitoring Exporter Set up the WebLogic Monitoring Exporter that will collect WebLogic Server metrics and monitor your WebCenter Sites domain.\nGenerate the WebLogic Monitoring Exporter Deployment Package Two packages are required as the listening ports are different for the Administration Server and Managed Servers. One binary required for the Admin Server (wls-exporter-as.war) and one for Managed Cluster (wls-exporter-ms.war). Set the required proxies and then run the script getX.X.X.sh to generate two binaries:\n$ cd kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter $ sh get1.1.0.sh Output:\n % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 607 0 607 0 0 357 0 --:--:-- 0:00:01 --:--:-- 357 100 2016k 100 2016k 0 0 398k 0 0:00:05 0:00:05 --:--:-- 797k -------------------wls-exporter-ms start------------------- created /tmp/ci-GNysQzP1kv Copying completed /tmp/ci-GNysQzP1kv /kubernetes/2.4.0/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter in temp dir adding: WEB-INF/weblogic.xml (deflated 66%) adding: config.yml (deflated 63%) wls-exporter-ms.war is ready -------------------wls-exporter-ms end------------------- -------------------wls-exporter-as start------------------- Copying completed in temp dir adding: WEB-INF/weblogic.xml (deflated 66%) adding: config.yml (deflated 52%) wls-exporter-as.war is ready -------------------wls-exporter-as end------------------- zip completed kubernetes/2.4.0/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter Copy the WAR Files to the WebLogic Domain Home Copy the wls-exporter-as.war and wls-exporter-ms.war files to the domain home directory in the Administration Server pod.\n$ kubectl cp wls-exporter-as.war wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ $ kubectl cp wls-exporter-ms.war wcsites-ns/wcsitesinfra-adminserver:/u01/oracle/user_projects/domains/wcsitesinfra/ Deploy the WebLogic Monitoring Exporter Follow these steps to deploy the package in the WebLogic Server instances:\n  In the Administration Server and Managed Servers, deploy the WebLogic Monitoring Exporter (wls-exporter-ms.war) separately using the Oracle Enterprise Manager.\n  Select the servers to which the Exporter WAR should be deployed:\n  Set the application name. The application name must be different if it is deployed separately in the Administration Server and Managed Servers. Make sure the context-root for both the deployments is wls-exporter:\n  Click Install and start application.\n  Then deploy the WebLogic Monitoring Exporter application (wls-exporter-ms.war).\n  Activate the changes to start the application. If the application is started and the port is exposed, then you can access the WebLogic Monitoring Exporter console using this URL: http://\u0026lt;server:port\u0026gt;/wls-exporter.\n  Repeat same steps for wls-exporter-as.war.\n  Configure Prometheus Operator Prometheus enables you to collect metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nSee the following sample service monitor deployment YAML configuration file located at\nkubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter/wls-exporter.yaml.\nServiceMonitor for wls-exporter:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= # Welcome1 i.e.'WebLogic password' user: d2VibG9naWM= # weblogic i.e. 'WebLogic username' type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-wcsitesinfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - wcsites-ns selector: matchLabels: weblogic.domainName: wcsitesinfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics The exporting of metrics from wls-exporter requires basicAuth so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret will be used in the ServiceMonitor deployment.\nWhen generating the base64 encoded strings for the user name and password, observe if a new line character is appended in the encoded string. This line character causes an authentication failure. To avoid a new line string, use the following example:\n$ echo -n \u0026quot;Welcome1\u0026quot; | base64 V2VsY29tZTE= In the deployment YAML configuration for wls-exporter shown above, weblogic.domainName: wcsitesinfra is used as a label under spec.selector.matchLabels, so all the services will be selected for the service monitor. If you don\u0026rsquo;t use this label, you should create separate service monitors for each server\u0026ndash;if the server name is used as matching labels in spec.selector.matchLabels. Doing so will require you to relabel the configuration because Prometheus, by default, ignores the labels provided in the wls-exporter.\nBy default, Prometheus does not store all the labels provided by the target. In the service monitor deployment YAML configuration, you must mention the relabeling configuration (spec.endpoints.relabelings) so that certain labels provided by weblogic-monitoring-exporter (required for the Grafana dashboard) are stored in Prometheus. Do not delete the following section from the configuration YAML file:\nrelabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) Add RoleBinding and Role for the WebLogic Domain Namespace The RoleBinding is required for Prometheus to access the endpoints provided by the WebLogic Monitoring Exporter. You need to add RoleBinding for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit the kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml file in the Prometheus Operator deployment manifests and add the RoleBinding for the namespace (wcsites-ns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: wcsites-ns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring Similarly, add the Role for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml in the Prometheus Operator deployment manifests and add the Role for the namespace (wcsites-ns) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: wcsites-ns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch Then apply prometheus-roleBindingSpecificNamespaces.yaml and prometheus-roleSpecificNamespaces.yaml for the RoleBinding and Role to take effect in the cluster.\n$ kubectl apply -f kube-prometheus/manifests/prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f kube-prometheus/manifests/prometheus-roleSpecificNamespaces.yaml Deploy the Service Monitor To deploy the service monitor, use the above wls-exporter.yaml deployment YAML and run the following command:\n$ kubectl create -f kubernetes/samples/scripts/create-wcsites-domain/utils/weblogic-monitoring-exporter/wls-exporter.yaml Additional Setup For Voyager Load Balancer In step 2 of Configure Voyager to Manage Ingresses, for wcsites-cluster, enable the last rule for path ‘wls-exporter’ and then re-deploy Voyager Load Balancer.\nEnable Prometheus to Discover the Service After the deployment of the service monitor, Prometheus should be able to discover wls-exporter and export metrics.\nYou can access the Prometheus dashboard at http://mycompany.com:32101/.\nDeploy Grafana Dashboard To view the domain metrics, deploy the Grafana dashboard provided in the WebLogic Monitoring Exporter.\nYou can access the Grafana dashboard at http://mycompany.com:32100/.\n  Log in to Grafana dashboard with admin/admin.\n  Go to Settings, then select DataSources, and then Add Data Source.\nHTTP URL: Prometheus URL http://mycompany.com:32101/\nAuth: Enable Basic Auth\nBasic Auth Details: Weblogic credentials provided in step Configure Prometheus Operator\n  Download the weblogic_dashboard.json file from here.\n  Click Add and then Import. Paste the modified JSON in the Paste JSON block, and then load it.\nThis displays the WebLogic Server Dashboard.\n  "
},
{
	"uri": "/fmw-kubernetes/wcsites-domains/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Recent changes    Date Version Introduces backward incompatibilities Change     2020-03-18 2.4.0 No Starting with Oracle WebCenter Sites 12.2.1.4.0 support with WebLogic Kubernetes Operator 2.4.0    "
},
{
	"uri": "/fmw-kubernetes/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]